{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioTag.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QColeman97/AudioTagger/blob/master/AudioTag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcgupmmSzS9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "42179c74-1261-425a-ebec-b9f495c654d1"
      },
      "source": [
        "# This cell contains not-preferred data pre-processing methods\n",
        "# This cell is left up for example, and to visualize spectrograms\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa\n",
        "from librosa import display\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "input_path = 'drive/My Drive/AudioTaggerData/'\n",
        "\n",
        "train_files_path = input_path + 'FSDKaggle2018.audio_train'\n",
        "test_files_path = input_path + 'FSDKaggle2018.audio_test'\n",
        "train_csv_path = (input_path +\n",
        "                  'FSDKaggle2018.meta/train_post_competition.csv')\n",
        "test_csv_path = (input_path +\n",
        "                 'FSDKaggle2018.meta/' +\n",
        "                 'test_post_competition_scoring_clips.csv')\n",
        "\n",
        "# Data preprocessing part\n",
        "\n",
        "df_train = pd.read_csv(train_csv_path)\n",
        "df_test = pd.read_csv(test_csv_path)\n",
        "\n",
        "unique_labels = df_train.label.unique()\n",
        "num_class = len(unique_labels)\n",
        "\n",
        "label2index = {label: index for index, label in enumerate(unique_labels)}\n",
        "\n",
        "train_dict = pd.Series.from_csv(train_csv_path, header = 0).to_dict()\n",
        "test_dict = pd.Series.from_csv(test_csv_path, header = 0).to_dict()\n",
        "\n",
        "#array of labels in number form (0 = hi-hat, 1 = saxophone, etc)\n",
        "label_emb_indices = np.array([label2index[label] for label in df_train.label])\n",
        "\n",
        "label_emb_test_indices = np.array([label2index[label] for label in df_test.label])\n",
        "\n",
        "\n",
        "def pre_process(pathname):\n",
        "    sampling_rate = 32000\n",
        "    hop_length = 192\n",
        "    fmax = None\n",
        "    n_mels = 128\n",
        "    n_fft = 1024\n",
        "\n",
        "    y, sr = librosa.load(pathname, sr = sampling_rate)\n",
        "    # \"Trim quiet noise away\" is somewhat effective\n",
        "    y, (trim_begin, trim_end) = librosa.effects.trim(y)\n",
        "    \n",
        "#     y = librosa.effects.time_stretch(y, 2.0)\n",
        "\n",
        "    # Amplitudes of STFT\n",
        "    stft = np.abs(librosa.stft(y, n_fft = n_fft, hop_length = hop_length,\n",
        "                               window = 'hann', center = True,\n",
        "                               pad_mode = 'reflect'))\n",
        "\n",
        "    freqs = librosa.core.fft_frequencies(sr = sampling_rate, n_fft = n_fft)\n",
        "    stft = librosa.perceptual_weighting(stft*2, freqs, ref = 1.0, amin = 1e-10,\n",
        "                                        top_db = 99.0)\n",
        "\n",
        "    # Apply mel filterbank\n",
        "    # Power param is set to 2 (power) by default\n",
        "    mel_spect = librosa.feature.melspectrogram(S = stft, sr = sampling_rate,\n",
        "                                               n_mels = n_mels, fmax = fmax)\n",
        "\n",
        "    log_mel_spect = librosa.core.power_to_db(mel_spect)\n",
        "\n",
        "    return np.asarray(log_mel_spect)\n",
        "\n",
        "\n",
        "# Get data as a numpy array from .wav files (not preferred, but left in for example)\n",
        "def get_data(pathname, training = True):\n",
        "    file_list = glob.glob(os.path.join(pathname, '*.wav'))\n",
        "\n",
        "    if training:\n",
        "        data_f = open('Audio.train', 'w')\n",
        "    else:\n",
        "        data_f = open('Audio.test', 'w')\n",
        "\n",
        "    spectrograms = np.ndarray((9474, 256, 128))\n",
        "\n",
        "    for i, file in enumerate(file_list):\n",
        "        print(\"%04d / %d | %s\" % (i + 1, len(file_list), file))\n",
        "\n",
        "        spectrogram = pre_process(file)\n",
        "\n",
        "        time_restriction = 256\n",
        "        if time_restriction >= spectrogram.shape[1]:\n",
        "            pad_amount = time_restriction - spectrogram.shape[1]\n",
        "            spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_amount)),\n",
        "                                 'minimum')\n",
        "        else:\n",
        "            spectrogram = spectrogram[:, :time_restriction]\n",
        "\n",
        "        spectrogram = spectrogram.transpose()\n",
        "\n",
        "        for j in range(len(spectrogram)):\n",
        "            for k in range(len(spectrogram[j])):\n",
        "                spectrograms[i][j][k] = spectrogram[j][k].astype(np.float32)\n",
        "\n",
        "\n",
        "        # Plot every 12th spectrogram\n",
        "        if i % 12 == 0:\n",
        "        \n",
        "            plt.figure(\"General-Purpose \")\n",
        "            plt.clf()\n",
        "            plt.subplots_adjust(right = 0.98, left = 0.1, bottom = 0.1,\n",
        "                                top = 0.99)\n",
        "            plt.imshow(spectrogram, origin = \"lower\",\n",
        "                       interpolation = \"nearest\", cmap = \"viridis\")\n",
        "            plt.xlabel(\"%d bins\" % spectrogram.shape[1])\n",
        "            plt.ylabel(\"%d frames\" % spectrogram.shape[0])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "\n",
        "# Get labels as a numpy array from .csv (not preferred, but left in for example)\n",
        "def get_labels(pathname, training = True):\n",
        "    file_list = glob.glob(os.path.join(pathname, '*.wav'))\n",
        "\n",
        "    if training:\n",
        "        labels_f = open('Labels.train', 'w')\n",
        "    else:\n",
        "        labels_f = open('Labels.test', 'w')\n",
        "\n",
        "    labels = np.ndarray((1570, 41))\n",
        "    for i, file in enumerate(file_list):\n",
        "        categ = (train_dict[file.split('/')[-1]] if\n",
        "            (training) else test_dict[file.split('/')[-1]])\n",
        "        hot_index = label2index[categ]\n",
        "        labels[i][hot_index] = 1\n",
        "\n",
        "    return np.array(labels)\n",
        "\n",
        "# Uncomment for example plotted spectrograms of training data\n",
        "# CAUTION: Will error if used with models\n",
        "# get_data(train_files_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4141: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
            "  infer_datetime_format=infer_datetime_format)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbbUGp4yGKIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afa9af02-c1c9-429a-b2f4-0bbcf751e17c"
      },
      "source": [
        "from keras import models, layers\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Model structure after best methods ~ 6 conv units (conv *2 + maxpool)\n",
        "def make_2D_CNN_model(input_shape):\n",
        "    # Example Shape: (500, 128, 1)  # 500 = timesteps, 128 = frequencies\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv2D(64, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu',\n",
        "                                  input_shape = input_shape))\n",
        "    # Shape: (126, 498, 64)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(64, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (124, 496, 64)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (62, 248, 64)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(128, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (60, 246, 128)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(128, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (58, 244, 128)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (29, 122, 128)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (25, 118, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (23, 116, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (21, 114, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (10, 57, 256)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (1, 24, 512)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "    return nn\n",
        "\n",
        "\n",
        "# Get data\n",
        "input_path = 'drive/My Drive/AudioTaggerData/'\n",
        "\n",
        "train_files_path = input_path + 'FSDKaggle2018.audio_train'\n",
        "test_files_path = input_path + 'FSDKaggle2018.audio_test'\n",
        "train_csv_path = (input_path +\n",
        "                  'FSDKaggle2018.meta/train_post_competition.csv')\n",
        "test_csv_path = (input_path +\n",
        "                 'FSDKaggle2018.meta/' +\n",
        "                 'test_post_competition_scoring_clips.csv')\n",
        "\n",
        "\n",
        "# Get train data as numpy array from training data file\n",
        "def get_train_data():\n",
        "#   Possible future better shape (9474, 256, 128))\n",
        "    data = np.ndarray((9474, 64, 32))\n",
        "\n",
        "    data_len, time_len, freq = 9474, 64, 32\n",
        "\n",
        "\n",
        "    with open(input_path + 'Audio.train', 'r') as data_f:\n",
        "        for i in range(data_len):\n",
        "\n",
        "            for j in range(time_len):\n",
        "    \n",
        "                time_step = [float(elem) for elem in\n",
        "                             data_f.readline().split()]\n",
        "                \n",
        "                if len(time_step) < freq:\n",
        "                    rest = freq - len(time_step)\n",
        "                    time_step += [-100.0 for x in range(rest)]\n",
        "    \n",
        "                for k in range(freq):\n",
        "                    data[i][j][k] = time_step[k]\n",
        "\n",
        "            data_f.readline()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Get test data as numpy array from test data file\n",
        "def get_test_data():\n",
        "#   Possible future better shape (9474, 256, 128))\n",
        "    data = np.ndarray((1570, 64, 32))\n",
        "    data_len = 1570\n",
        "\n",
        "    with open(input_path + 'Audio.test', 'r') as data_f:\n",
        "        for i in range(data_len):\n",
        "\n",
        "            for j in range(64):\n",
        "    \n",
        "                time_step = [float(elem) for elem in\n",
        "                             data_f.readline().split()]\n",
        "                \n",
        "                if len(time_step) < 32:\n",
        "                    rest = 32 - len(time_step)\n",
        "                    time_step += [-100.0 for x in range(rest)]\n",
        "    \n",
        "                for k in range(32):\n",
        "                    data[i][j][k] = time_step[k]\n",
        "\n",
        "            data_f.readline()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Get train labels as numpy array from training labels file\n",
        "def get_train_labels():\n",
        "    labels = np.ndarray((9474, 41))\n",
        "    labels_len = 9474\n",
        "\n",
        "    with open(input_path + 'Labels.train', 'r') as label_f:\n",
        "        for i in range(labels_len):\n",
        "            label = [float(elem) for elem in label_f.readline().split()]\n",
        "\n",
        "            if len(label) < 41:\n",
        "                    rest = 41 - len(label)\n",
        "                    label += [0.0 for x in range(rest)]\n",
        "            \n",
        "            for j in range(41):\n",
        "                labels[i][j] = label[j]\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "# Get test labels as numpy array from test labels file\n",
        "def get_test_labels():\n",
        "    labels = np.ndarray((1570, 41))\n",
        "    labels_len = 1570\n",
        "\n",
        "    with open(input_path + 'Labels.test', 'r') as label_f:\n",
        "        for i in range(labels_len):\n",
        "            label = [float(elem) for elem in label_f.readline().split()]\n",
        "\n",
        "            if len(label) < 41:\n",
        "                    rest = 41 - len(label)\n",
        "                    label += [0.0 for x in range(rest)]\n",
        "            \n",
        "            for j in range(41):\n",
        "                labels[i][j] = label[j]\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "# Data\n",
        "train_data = get_train_data()\n",
        "train_labels = get_train_labels()\n",
        "# Labels\n",
        "test_data = get_test_data()\n",
        "test_labels = get_test_labels()\n",
        "\n",
        "\n",
        "train_samples, test_samples = 9474, 1570\n",
        "max_timesteps = train_data.shape[1]\n",
        "num_freq = train_data.shape[2]\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ua6ZrXRNkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1805
        },
        "outputId": "97c0cada-fadf-4686-c214-f4d2e7a6e337"
      },
      "source": [
        "mean = train_data.mean(axis = 0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std\n",
        "\n",
        "\n",
        "model = make_2D_CNN_model((max_timesteps, num_freq, 1))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    partial_train_data = partial_train_data.reshape(\n",
        "            (partial_train_data.shape[0], \n",
        "             partial_train_data.shape[1], \n",
        "             partial_train_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    val_data = val_data.reshape(\n",
        "            (val_data.shape[0], \n",
        "             val_data.shape[1], \n",
        "             val_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('2DCNN.h5')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 19s 3ms/step - loss: 3.4796 - acc: 0.0968 - val_loss: 3.7399 - val_acc: 0.0819\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 5s 770us/step - loss: 3.0668 - acc: 0.1749 - val_loss: 3.2972 - val_acc: 0.1609\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 5s 770us/step - loss: 2.9031 - acc: 0.2131 - val_loss: 3.3685 - val_acc: 0.1731\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 776us/step - loss: 2.7443 - acc: 0.2519 - val_loss: 3.0756 - val_acc: 0.2272\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 777us/step - loss: 2.6468 - acc: 0.2608 - val_loss: 3.0121 - val_acc: 0.2293\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 778us/step - loss: 2.5838 - acc: 0.2799 - val_loss: 3.1912 - val_acc: 0.2247\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 783us/step - loss: 2.5266 - acc: 0.3020 - val_loss: 2.7806 - val_acc: 0.2606\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 785us/step - loss: 2.4191 - acc: 0.3232 - val_loss: 2.9903 - val_acc: 0.2470\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 786us/step - loss: 2.3941 - acc: 0.3366 - val_loss: 3.0713 - val_acc: 0.2487\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 791us/step - loss: 2.3322 - acc: 0.3491 - val_loss: 2.7345 - val_acc: 0.3041\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 792us/step - loss: 2.3707 - acc: 0.3431 - val_loss: 2.5277 - val_acc: 0.3345\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 798us/step - loss: 2.3034 - acc: 0.3590 - val_loss: 2.4138 - val_acc: 0.3619\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 803us/step - loss: 2.2495 - acc: 0.3726 - val_loss: 2.4690 - val_acc: 0.3543\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 806us/step - loss: 2.2299 - acc: 0.3757 - val_loss: 2.3688 - val_acc: 0.3670\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 2.1701 - acc: 0.3902 - val_loss: 2.4287 - val_acc: 0.3720\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 813us/step - loss: 2.1573 - acc: 0.3988 - val_loss: 2.3864 - val_acc: 0.3699\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 815us/step - loss: 2.1097 - acc: 0.3992 - val_loss: 2.3141 - val_acc: 0.3691\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 814us/step - loss: 2.0911 - acc: 0.4075 - val_loss: 2.2629 - val_acc: 0.4012\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 813us/step - loss: 2.0468 - acc: 0.4249 - val_loss: 2.4463 - val_acc: 0.3712\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 2.0117 - acc: 0.4281 - val_loss: 2.3755 - val_acc: 0.3720\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 803us/step - loss: 2.0678 - acc: 0.4229 - val_loss: 2.1239 - val_acc: 0.4269\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 801us/step - loss: 2.0225 - acc: 0.4272 - val_loss: 1.9879 - val_acc: 0.4472\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 799us/step - loss: 1.9817 - acc: 0.4434 - val_loss: 2.1321 - val_acc: 0.4350\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 799us/step - loss: 1.9555 - acc: 0.4486 - val_loss: 1.9719 - val_acc: 0.4569\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 799us/step - loss: 1.9461 - acc: 0.4523 - val_loss: 2.1381 - val_acc: 0.4126\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 798us/step - loss: 1.8987 - acc: 0.4651 - val_loss: 2.1066 - val_acc: 0.4345\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 801us/step - loss: 1.8841 - acc: 0.4637 - val_loss: 2.1766 - val_acc: 0.4253\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 801us/step - loss: 1.8515 - acc: 0.4706 - val_loss: 2.2715 - val_acc: 0.4113\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 802us/step - loss: 1.8311 - acc: 0.4758 - val_loss: 2.1027 - val_acc: 0.4375\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 804us/step - loss: 1.8043 - acc: 0.4834 - val_loss: 2.1411 - val_acc: 0.4354\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 802us/step - loss: 1.8641 - acc: 0.4695 - val_loss: 1.8422 - val_acc: 0.4916\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 1.8367 - acc: 0.4744 - val_loss: 1.8318 - val_acc: 0.4810\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 1.8024 - acc: 0.4838 - val_loss: 1.7261 - val_acc: 0.5156\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 1.7561 - acc: 0.5035 - val_loss: 1.7281 - val_acc: 0.5063\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 807us/step - loss: 1.7371 - acc: 0.5023 - val_loss: 1.7933 - val_acc: 0.5004\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 803us/step - loss: 1.6949 - acc: 0.5145 - val_loss: 1.8425 - val_acc: 0.4764\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 803us/step - loss: 1.6926 - acc: 0.5153 - val_loss: 1.8807 - val_acc: 0.4683\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 804us/step - loss: 1.6779 - acc: 0.5205 - val_loss: 1.8534 - val_acc: 0.4759\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 803us/step - loss: 1.6444 - acc: 0.5311 - val_loss: 1.9294 - val_acc: 0.4785\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 802us/step - loss: 1.6197 - acc: 0.5324 - val_loss: 1.8742 - val_acc: 0.4886\n",
            "0.33372043858508804 / 2.558421367729032\n",
            "0.3627533796872642 / 2.3826772143711916\n",
            "0.36951013594060333 / 2.4239236261393575\n",
            "0.3893581090142598 / 2.2861152073821502\n",
            "0.3785895278324952 / 2.3430209522311753\n",
            "0.3763724661155327 / 2.381663227403486\n",
            "0.38080658642826853 / 2.287995601828034\n",
            "0.38386824294119265 / 2.344511120705991\n",
            "0.38397381837303574 / 2.3874235958666414\n",
            "0.40002111567033305 / 2.2813247329479935\n",
            "Best val loss: 2.558421367729032 & with acc: 0.33372043858508804 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcabAPVsZpEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        },
        "outputId": "8df3e3d2-90ae-4500-ad2c-88e6bbb60a8f"
      },
      "source": [
        "\n",
        "def make_combined_CNN_RNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu',\n",
        "                                  input_shape = (None, input_shape[-1])))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling1D(3))\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3)))\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "model = make_combined_CNN_RNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('1DCNN_RNN.h5')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 11s 2ms/step - loss: 3.5553 - acc: 0.0767 - val_loss: 3.3537 - val_acc: 0.1170\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 1s 200us/step - loss: 3.2876 - acc: 0.1184 - val_loss: 3.2230 - val_acc: 0.1432\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 1s 202us/step - loss: 3.1506 - acc: 0.1472 - val_loss: 3.1319 - val_acc: 0.1672\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 1s 208us/step - loss: 3.0514 - acc: 0.1689 - val_loss: 3.1618 - val_acc: 0.1668\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 1s 203us/step - loss: 2.9963 - acc: 0.1739 - val_loss: 3.0037 - val_acc: 0.1917\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 1s 206us/step - loss: 2.8857 - acc: 0.2063 - val_loss: 2.8998 - val_acc: 0.2137\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 1s 203us/step - loss: 2.8556 - acc: 0.2076 - val_loss: 2.8448 - val_acc: 0.2302\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 1s 206us/step - loss: 2.7945 - acc: 0.2250 - val_loss: 2.8728 - val_acc: 0.2297\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 1s 202us/step - loss: 2.7628 - acc: 0.2314 - val_loss: 2.7942 - val_acc: 0.2416\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 1s 203us/step - loss: 2.7005 - acc: 0.2467 - val_loss: 2.7734 - val_acc: 0.2394\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 1s 206us/step - loss: 2.7119 - acc: 0.2498 - val_loss: 2.5907 - val_acc: 0.2745\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 1s 204us/step - loss: 2.6777 - acc: 0.2565 - val_loss: 2.5675 - val_acc: 0.2880\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 1s 204us/step - loss: 2.6496 - acc: 0.2580 - val_loss: 2.5894 - val_acc: 0.2686\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 1s 202us/step - loss: 2.6145 - acc: 0.2661 - val_loss: 2.5633 - val_acc: 0.2893\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 1s 205us/step - loss: 2.5880 - acc: 0.2754 - val_loss: 2.5606 - val_acc: 0.2838\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 1s 209us/step - loss: 2.5564 - acc: 0.2802 - val_loss: 2.5569 - val_acc: 0.2905\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 1s 211us/step - loss: 2.5442 - acc: 0.2860 - val_loss: 2.4941 - val_acc: 0.3079\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 1s 204us/step - loss: 2.5084 - acc: 0.2962 - val_loss: 2.4873 - val_acc: 0.3125\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 1s 209us/step - loss: 2.4881 - acc: 0.3026 - val_loss: 2.5116 - val_acc: 0.3095\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 2s 211us/step - loss: 2.4620 - acc: 0.3000 - val_loss: 2.5070 - val_acc: 0.3121\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 1s 207us/step - loss: 2.4939 - acc: 0.2964 - val_loss: 2.3416 - val_acc: 0.3370\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 1s 208us/step - loss: 2.4678 - acc: 0.3014 - val_loss: 2.3614 - val_acc: 0.3311\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 1s 204us/step - loss: 2.4393 - acc: 0.3100 - val_loss: 2.3415 - val_acc: 0.3298\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 1s 206us/step - loss: 2.4281 - acc: 0.3152 - val_loss: 2.3758 - val_acc: 0.3294\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 1s 202us/step - loss: 2.3965 - acc: 0.3217 - val_loss: 2.3324 - val_acc: 0.3539\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 1s 206us/step - loss: 2.3885 - acc: 0.3200 - val_loss: 2.3494 - val_acc: 0.3412\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 1s 205us/step - loss: 2.3804 - acc: 0.3262 - val_loss: 2.3668 - val_acc: 0.3302\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 1s 205us/step - loss: 2.3585 - acc: 0.3308 - val_loss: 2.4335 - val_acc: 0.3171\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 1s 203us/step - loss: 2.3463 - acc: 0.3351 - val_loss: 2.3606 - val_acc: 0.3421\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 1s 206us/step - loss: 2.3251 - acc: 0.3348 - val_loss: 2.3341 - val_acc: 0.3488\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 1s 204us/step - loss: 2.3576 - acc: 0.3321 - val_loss: 2.1277 - val_acc: 0.3746\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 1s 205us/step - loss: 2.3490 - acc: 0.3339 - val_loss: 2.1583 - val_acc: 0.3725\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 1s 207us/step - loss: 2.3227 - acc: 0.3401 - val_loss: 2.1130 - val_acc: 0.3923\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 1s 208us/step - loss: 2.2970 - acc: 0.3442 - val_loss: 2.1463 - val_acc: 0.3868\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 1s 208us/step - loss: 2.2942 - acc: 0.3425 - val_loss: 2.1416 - val_acc: 0.3898\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 1s 208us/step - loss: 2.2802 - acc: 0.3527 - val_loss: 2.1902 - val_acc: 0.3809\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 1s 204us/step - loss: 2.2641 - acc: 0.3563 - val_loss: 2.1914 - val_acc: 0.3763\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 1s 203us/step - loss: 2.2397 - acc: 0.3583 - val_loss: 2.1828 - val_acc: 0.3834\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 1s 205us/step - loss: 2.2332 - acc: 0.3607 - val_loss: 2.2129 - val_acc: 0.3699\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 1s 202us/step - loss: 2.2235 - acc: 0.3656 - val_loss: 2.1700 - val_acc: 0.3894\n",
            "0.2757601355378692 / 2.6034641845806226\n",
            "0.28367821036561114 / 2.5775513471783817\n",
            "0.28948479820344897 / 2.543949747407759\n",
            "0.29307432442500786 / 2.5618020312206164\n",
            "0.3047930740222738 / 2.5096061519674353\n",
            "0.30658783803920486 / 2.499055097231994\n",
            "0.3111275339851508 / 2.4742559259002275\n",
            "0.3107052359830689 / 2.4941106921917684\n",
            "0.31577280375200345 / 2.4698457186286515\n",
            "0.3224239856810183 / 2.44613068490415\n",
            "Best val loss: 2.6034641845806226 & with acc: 0.2757601355378692 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UODbmGDa-LH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1686
        },
        "outputId": "b3479b32-3c56-4923-9a2a-b5b77ad03ee2"
      },
      "source": [
        "from keras.applications import DenseNet121\n",
        "\n",
        "def make_dense_net_model(conv_base):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(conv_base)\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "    return nn\n",
        "\n",
        "dn_base = DenseNet121(include_top = False,\n",
        "                      input_shape = (max_timesteps, num_freq, 3),\n",
        "                      pooling = 'avg')\n",
        "dn_base.trainable = True\n",
        "# print(dn_base.summary())\n",
        "# Fine-tuning\n",
        "set_trainable = False\n",
        "for layer in dn_base.layers:\n",
        "    if layer.name == 'conv5_block13_0_bn':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "model = make_dense_net_model(dn_base)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    partial_train_data = partial_train_data.reshape(\n",
        "            (partial_train_data.shape[0], \n",
        "             partial_train_data.shape[1], \n",
        "             partial_train_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    partial_train_data = np.repeat(partial_train_data, 3, axis=3)\n",
        "    \n",
        "    val_data = val_data.reshape(\n",
        "            (val_data.shape[0], \n",
        "             val_data.shape[1], \n",
        "             val_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    val_data = np.repeat(val_data, 3, axis=3)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('DenseNetModel.h5')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 1s 0us/step\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 27s 4ms/step - loss: 3.8942 - acc: 0.1044 - val_loss: 10.9026 - val_acc: 0.0507\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 2s 305us/step - loss: 2.7456 - acc: 0.2765 - val_loss: 10.3855 - val_acc: 0.0490\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 2s 307us/step - loss: 2.2430 - acc: 0.4039 - val_loss: 10.6178 - val_acc: 0.0553\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 2s 306us/step - loss: 1.8583 - acc: 0.5151 - val_loss: 10.5807 - val_acc: 0.0498\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 2s 307us/step - loss: 1.5388 - acc: 0.6227 - val_loss: 10.4711 - val_acc: 0.0541\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 2s 307us/step - loss: 1.2544 - acc: 0.7170 - val_loss: 10.7524 - val_acc: 0.0524\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 2s 308us/step - loss: 1.0458 - acc: 0.7791 - val_loss: 10.7840 - val_acc: 0.0448\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 2s 309us/step - loss: 0.8665 - acc: 0.8265 - val_loss: 10.6212 - val_acc: 0.0515\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 2s 310us/step - loss: 0.7408 - acc: 0.8538 - val_loss: 10.3876 - val_acc: 0.0524\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 2s 309us/step - loss: 0.6322 - acc: 0.8749 - val_loss: 10.7435 - val_acc: 0.0473\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 2s 314us/step - loss: 1.4807 - acc: 0.6623 - val_loss: 11.3679 - val_acc: 0.0460\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 2s 312us/step - loss: 0.9930 - acc: 0.7626 - val_loss: 10.7727 - val_acc: 0.0435\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 2s 313us/step - loss: 0.7685 - acc: 0.8249 - val_loss: 10.7450 - val_acc: 0.0456\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 2s 312us/step - loss: 0.6532 - acc: 0.8558 - val_loss: 10.6904 - val_acc: 0.0519\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 2s 314us/step - loss: 0.5656 - acc: 0.8717 - val_loss: 11.0453 - val_acc: 0.0494\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 2s 312us/step - loss: 0.5419 - acc: 0.8746 - val_loss: 11.3217 - val_acc: 0.0490\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 2s 310us/step - loss: 0.5088 - acc: 0.8760 - val_loss: 11.4251 - val_acc: 0.0570\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 2s 309us/step - loss: 0.4892 - acc: 0.8777 - val_loss: 11.8440 - val_acc: 0.0549\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 2s 309us/step - loss: 0.4834 - acc: 0.8757 - val_loss: 12.1663 - val_acc: 0.0536\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 2s 310us/step - loss: 0.4690 - acc: 0.8781 - val_loss: 11.6859 - val_acc: 0.0494\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 2s 307us/step - loss: 0.6829 - acc: 0.8202 - val_loss: 12.2169 - val_acc: 0.0617\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 2s 307us/step - loss: 0.5055 - acc: 0.8679 - val_loss: 12.3451 - val_acc: 0.0549\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 2s 305us/step - loss: 0.4788 - acc: 0.8735 - val_loss: 12.3915 - val_acc: 0.0587\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 2s 305us/step - loss: 0.4746 - acc: 0.8752 - val_loss: 12.2612 - val_acc: 0.0595\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 2s 304us/step - loss: 0.4647 - acc: 0.8757 - val_loss: 12.2779 - val_acc: 0.0570\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 2s 305us/step - loss: 0.4626 - acc: 0.8756 - val_loss: 12.5656 - val_acc: 0.0595\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 2s 303us/step - loss: 0.4557 - acc: 0.8753 - val_loss: 12.3464 - val_acc: 0.0650\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 2s 305us/step - loss: 0.4619 - acc: 0.8759 - val_loss: 13.1414 - val_acc: 0.0600\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 2s 305us/step - loss: 0.4606 - acc: 0.8704 - val_loss: 13.3308 - val_acc: 0.0617\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 2s 304us/step - loss: 0.4587 - acc: 0.8726 - val_loss: 13.0449 - val_acc: 0.0659\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 2s 303us/step - loss: 0.5054 - acc: 0.8672 - val_loss: 13.6802 - val_acc: 0.0410\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 2s 303us/step - loss: 0.4531 - acc: 0.8778 - val_loss: 13.3254 - val_acc: 0.0574\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 2s 302us/step - loss: 0.4400 - acc: 0.8786 - val_loss: 13.4554 - val_acc: 0.0553\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 2s 303us/step - loss: 0.4378 - acc: 0.8769 - val_loss: 13.2883 - val_acc: 0.0566\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 2s 302us/step - loss: 0.4395 - acc: 0.8771 - val_loss: 13.8508 - val_acc: 0.0591\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 2s 303us/step - loss: 0.4384 - acc: 0.8771 - val_loss: 13.5561 - val_acc: 0.0562\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 2s 305us/step - loss: 0.4347 - acc: 0.8794 - val_loss: 13.9336 - val_acc: 0.0503\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 2s 304us/step - loss: 0.4353 - acc: 0.8788 - val_loss: 13.7327 - val_acc: 0.0604\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 2s 304us/step - loss: 0.4350 - acc: 0.8771 - val_loss: 14.1852 - val_acc: 0.0608\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 2s 306us/step - loss: 0.4275 - acc: 0.8801 - val_loss: 14.1512 - val_acc: 0.0515\n",
            "0.04983108095522668 / 12.04189309558353\n",
            "0.051203547448322576 / 11.70717437202866\n",
            "0.05373733098039756 / 11.802424456622148\n",
            "0.05447635140169311 / 11.705132426442326\n",
            "0.054898648749332174 / 11.911289253750361\n",
            "0.054265202602019176 / 12.048959815824354\n",
            "0.05426520267753183 / 12.122277575570184\n",
            "0.05669341206147865 / 12.334810450270368\n",
            "0.05711570938394682 / 12.517475385923643\n",
            "0.05352618250794507 / 12.406396028157827\n",
            "Best val loss: 12.04189309558353 & with acc: 0.04983108095522668 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RimUBVXglSku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        },
        "outputId": "03ff4556-c60a-47ed-cdfa-d6d8f58f58c2"
      },
      "source": [
        "\n",
        "\n",
        "def make_1DCNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu',\n",
        "                                  input_shape = (None, input_shape[-1])))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling1D(5))\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.GlobalAveragePooling1D())\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "model = make_1DCNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('1DCNN.h5')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 7s 978us/step - loss: 3.6705 - acc: 0.0754 - val_loss: 3.4182 - val_acc: 0.1318\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 3.3167 - acc: 0.1372 - val_loss: 3.2909 - val_acc: 0.1567\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 3.1784 - acc: 0.1665 - val_loss: 3.2103 - val_acc: 0.1613\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 3.0847 - acc: 0.1828 - val_loss: 3.1519 - val_acc: 0.1812\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 3.0115 - acc: 0.1936 - val_loss: 3.0955 - val_acc: 0.1820\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.9392 - acc: 0.2070 - val_loss: 3.0488 - val_acc: 0.1981\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.8827 - acc: 0.2194 - val_loss: 3.0270 - val_acc: 0.2052\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.8331 - acc: 0.2322 - val_loss: 2.9653 - val_acc: 0.2154\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.7832 - acc: 0.2467 - val_loss: 2.9792 - val_acc: 0.2162\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.7288 - acc: 0.2570 - val_loss: 2.9473 - val_acc: 0.2272\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.7720 - acc: 0.2461 - val_loss: 2.6562 - val_acc: 0.2791\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.7257 - acc: 0.2580 - val_loss: 2.6484 - val_acc: 0.2884\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.6802 - acc: 0.2731 - val_loss: 2.6486 - val_acc: 0.2897\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.6392 - acc: 0.2782 - val_loss: 2.5994 - val_acc: 0.3011\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.6008 - acc: 0.2950 - val_loss: 2.6682 - val_acc: 0.2893\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.5733 - acc: 0.2978 - val_loss: 2.6297 - val_acc: 0.2893\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.5463 - acc: 0.3031 - val_loss: 2.5702 - val_acc: 0.3045\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.5046 - acc: 0.3126 - val_loss: 2.5723 - val_acc: 0.3104\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.4734 - acc: 0.3197 - val_loss: 2.5195 - val_acc: 0.3197\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.4513 - acc: 0.3307 - val_loss: 2.5317 - val_acc: 0.3155\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.4883 - acc: 0.3159 - val_loss: 2.4176 - val_acc: 0.3480\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.4507 - acc: 0.3323 - val_loss: 2.3217 - val_acc: 0.3636\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.4311 - acc: 0.3365 - val_loss: 2.3216 - val_acc: 0.3632\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.3972 - acc: 0.3465 - val_loss: 2.3347 - val_acc: 0.3716\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.3804 - acc: 0.3393 - val_loss: 2.3199 - val_acc: 0.3695\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.3576 - acc: 0.3500 - val_loss: 2.3206 - val_acc: 0.3611\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.3330 - acc: 0.3619 - val_loss: 2.3733 - val_acc: 0.3509\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.3093 - acc: 0.3643 - val_loss: 2.3633 - val_acc: 0.3514\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.2834 - acc: 0.3701 - val_loss: 2.3636 - val_acc: 0.3615\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.2764 - acc: 0.3708 - val_loss: 2.3244 - val_acc: 0.3568\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.3069 - acc: 0.3598 - val_loss: 2.0972 - val_acc: 0.4236\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.2973 - acc: 0.3634 - val_loss: 2.0966 - val_acc: 0.4231\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.2610 - acc: 0.3700 - val_loss: 2.1270 - val_acc: 0.4227\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.2538 - acc: 0.3741 - val_loss: 2.1842 - val_acc: 0.4063\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.2230 - acc: 0.3824 - val_loss: 2.1257 - val_acc: 0.4092\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 44us/step - loss: 2.1968 - acc: 0.3867 - val_loss: 2.1801 - val_acc: 0.4020\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.1936 - acc: 0.3880 - val_loss: 2.1523 - val_acc: 0.4050\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.1727 - acc: 0.3883 - val_loss: 2.1587 - val_acc: 0.4037\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 45us/step - loss: 2.1710 - acc: 0.3819 - val_loss: 2.1556 - val_acc: 0.3991\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 46us/step - loss: 2.1430 - acc: 0.4008 - val_loss: 2.2056 - val_acc: 0.3986\n",
            "0.2956081070509311 / 2.647304233667013\n",
            "0.30796030405405406 / 2.589402749731734\n",
            "0.3092271951404778 / 2.576870792620891\n",
            "0.31503378368310025 / 2.5675474050882703\n",
            "0.31249999919453186 / 2.552316548051061\n",
            "0.3126055744250078 / 2.5447949863768913\n",
            "0.31640624959726593 / 2.530710315382158\n",
            "0.32020692577635923 / 2.514878749847412\n",
            "0.32411317567567566 / 2.504503085806563\n",
            "0.32453547287228945 / 2.502244570770779\n",
            "Best val loss: 2.647304233667013 & with acc: 0.2956081070509311 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCOWN6mzmJBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        },
        "outputId": "43c1db84-2ea3-4446-cbb1-6c435cd238be"
      },
      "source": [
        "def make_RNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True,\n",
        "                                            input_shape =\n",
        "                                            (None, input_shape[-1]))))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3)))\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "\n",
        "model = make_RNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('RNN.h5')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 19s 3ms/step - loss: 3.6041 - acc: 0.0628 - val_loss: 3.4624 - val_acc: 0.1263\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 3.3833 - acc: 0.1217 - val_loss: 3.3087 - val_acc: 0.1427\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 3.2540 - acc: 0.1455 - val_loss: 3.2218 - val_acc: 0.1567\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 3.1634 - acc: 0.1635 - val_loss: 3.1521 - val_acc: 0.1769\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 3.1039 - acc: 0.1751 - val_loss: 3.1101 - val_acc: 0.1845\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 3.0487 - acc: 0.1845 - val_loss: 3.0731 - val_acc: 0.1867\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 3.0116 - acc: 0.1921 - val_loss: 3.0280 - val_acc: 0.1926\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.9689 - acc: 0.2043 - val_loss: 3.0021 - val_acc: 0.1934\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.9431 - acc: 0.2067 - val_loss: 2.9678 - val_acc: 0.2031\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.9095 - acc: 0.2178 - val_loss: 2.9581 - val_acc: 0.2107\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.9375 - acc: 0.2057 - val_loss: 2.7579 - val_acc: 0.2593\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.9200 - acc: 0.2133 - val_loss: 2.7531 - val_acc: 0.2483\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.8930 - acc: 0.2163 - val_loss: 2.7392 - val_acc: 0.2563\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.8767 - acc: 0.2191 - val_loss: 2.7269 - val_acc: 0.2665\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.8464 - acc: 0.2261 - val_loss: 2.7129 - val_acc: 0.2635\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.8479 - acc: 0.2263 - val_loss: 2.7124 - val_acc: 0.2652\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.8176 - acc: 0.2342 - val_loss: 2.6866 - val_acc: 0.2715\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.8115 - acc: 0.2347 - val_loss: 2.6849 - val_acc: 0.2677\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7987 - acc: 0.2378 - val_loss: 2.6730 - val_acc: 0.2758\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7908 - acc: 0.2381 - val_loss: 2.6462 - val_acc: 0.2753\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.8009 - acc: 0.2430 - val_loss: 2.5973 - val_acc: 0.2758\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7713 - acc: 0.2368 - val_loss: 2.6055 - val_acc: 0.2694\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7574 - acc: 0.2487 - val_loss: 2.5992 - val_acc: 0.2808\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7351 - acc: 0.2498 - val_loss: 2.6009 - val_acc: 0.2644\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7298 - acc: 0.2530 - val_loss: 2.5881 - val_acc: 0.2800\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7082 - acc: 0.2508 - val_loss: 2.5913 - val_acc: 0.2690\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.7113 - acc: 0.2518 - val_loss: 2.5922 - val_acc: 0.2804\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6822 - acc: 0.2647 - val_loss: 2.5736 - val_acc: 0.2834\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6753 - acc: 0.2667 - val_loss: 2.5713 - val_acc: 0.2859\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6734 - acc: 0.2677 - val_loss: 2.5434 - val_acc: 0.2863\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6778 - acc: 0.2693 - val_loss: 2.5037 - val_acc: 0.3045\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6603 - acc: 0.2703 - val_loss: 2.4912 - val_acc: 0.3066\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6487 - acc: 0.2726 - val_loss: 2.4970 - val_acc: 0.3079\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6409 - acc: 0.2775 - val_loss: 2.5026 - val_acc: 0.3108\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6290 - acc: 0.2733 - val_loss: 2.5122 - val_acc: 0.2998\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6152 - acc: 0.2810 - val_loss: 2.5010 - val_acc: 0.3138\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.5993 - acc: 0.2833 - val_loss: 2.4980 - val_acc: 0.3108\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.6007 - acc: 0.2813 - val_loss: 2.4824 - val_acc: 0.3163\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.5896 - acc: 0.2878 - val_loss: 2.4878 - val_acc: 0.3197\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 7s 1ms/step - loss: 2.5771 - acc: 0.2889 - val_loss: 2.4758 - val_acc: 0.3142\n",
            "0.2414484797800715 / 2.830292846705462\n",
            "0.2417652027027027 / 2.789620895643492\n",
            "0.2504222977000314 / 2.7642753237002604\n",
            "0.25464526976685264 / 2.7456509454830273\n",
            "0.25696790490198784 / 2.730817353403246\n",
            "0.2586570945945946 / 2.7194588877059314\n",
            "0.26383023578170184 / 2.7012164528305465\n",
            "0.2652027022999686 / 2.685756960430661\n",
            "0.27111486536828244 / 2.674971627222525\n",
            "0.2716427368892206 / 2.6558943713033525\n",
            "Best val loss: 2.830292846705462 & with acc: 0.2414484797800715 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf8hLixeHaDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_2D = test_data.reshape(\n",
        "               (test_data.shape[0], \n",
        "                test_data.shape[1], \n",
        "                test_data.shape[2], \n",
        "                1))\n",
        "    \n",
        "test_data_2D_3chan = np.repeat(test_data_2D, 3, axis=3)\n",
        "    \n",
        "\n",
        "cnn2D = models.load_model('2DCNN.h5')\n",
        "rnn = models.load_model('RNN.h5')\n",
        "cnn1D = models.load_model('1DCNN.h5')\n",
        "cnn_rnn = models.load_model('1DCNN_RNN.h5')\n",
        "dense_net = models.load_model('DenseNetModel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6T_IeSRm0ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn2D_preds = cnn2D.predict(test_data_2D)\n",
        "rnn_preds = rnn.predict(test_data)\n",
        "cnn1D_preds = cnn1D.predict(test_data)\n",
        "cnn_rnn_preds = cnn_rnn.predict(test_data)\n",
        "dense_net_preds = dense_net.predict(test_data_2D_3chan)\n",
        "\n",
        "final_preds = (0.5 * cnn2D_preds + 0.08 * rnn_preds + 0.08 * cnn1D_preds + \n",
        "              0.08 * cnn_rnn_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68cI9a2I_sG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5fba6e01-9cc9-4729-8209-d856f594149d"
      },
      "source": [
        "\n",
        "with open('TestTags.out', 'w') as ttf:\n",
        "    test_file_list = glob.glob(os.path.join(test_files_path, '*.wav'))\n",
        "    for i, filename in enumerate(test_file_list[:1570]):\n",
        "        ttf.write(str(i) + ') ' + filename.split('/')[-1] + ' ' + \n",
        "                  unique_labels[np.argmax(final_preds[i])] + '\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.79205555e-02 6.77923963e-04 2.38153320e-02 ... 1.28061080e-03\n",
            "  1.20461974e-02 3.13588306e-02]\n",
            " [1.79205555e-02 6.77923963e-04 2.38153320e-02 ... 1.28061080e-03\n",
            "  1.20461974e-02 3.13588306e-02]\n",
            " [5.01193354e-05 3.25891771e-03 2.97569972e-03 ... 9.69696674e-04\n",
            "  6.53710589e-03 8.01199712e-05]\n",
            " ...\n",
            " [9.35527217e-03 1.05663610e-03 1.25318849e-02 ... 1.23593386e-03\n",
            "  1.35002118e-02 2.34928131e-02]\n",
            " [1.13905146e-04 2.67323405e-02 1.28292982e-02 ... 1.32288353e-03\n",
            "  8.24602097e-02 6.66971493e-04]\n",
            " [1.05594366e-03 4.05793637e-02 1.12186028e-02 ... 1.05384178e-02\n",
            "  7.65757337e-02 8.70423450e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5QoCT-gF8te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dc4ca8da-e3d3-4cda-972d-5122acd451bf"
      },
      "source": [
        "cnn2D_loss, cnn2D_acc = cnn2D.evaluate(test_data_2D, test_labels)\n",
        "rnn_loss, rnn_acc = rnn.evaluate(test_data, test_labels)\n",
        "cnn1D_loss, cnn1D_acc = cnn1D.evaluate(test_data, test_labels)\n",
        "cnn_rnn_loss, cnn_rnn_acc = cnn_rnn.evaluate(test_data, test_labels)\n",
        "dense_net_loss, dense_net_acc = dense_net.evaluate(test_data_2D_3chan, test_labels)\n",
        "\n",
        "print('2DCNN Test Accuracy:', str(cnn2D_acc*100) + '%')\n",
        "print('1DCNN Test Accuracy:', str(cnn1D_acc*100) + '%')\n",
        "print('Combined 1D CNN & RNN Test Accuracy:', str(cnn_rnn_acc*100) + '%')\n",
        "print('Dense Net Conv. Base + Classifier Test Accuracy:', str(dense_net_acc*100) + '%')\n",
        "print('RNN Test Accuracy:', str(rnn_acc*100) + '%')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1570/1570 [==============================] - 9s 5ms/step\n",
            "1570/1570 [==============================] - 27s 17ms/step\n",
            "1570/1570 [==============================] - 8s 5ms/step\n",
            "1570/1570 [==============================] - 10s 7ms/step\n",
            "1570/1570 [==============================] - 10s 6ms/step\n",
            "2DCNN Test Loss: 45.6687898089172%\n",
            "1DCNN Test Loss: 31.27388535031847%\n",
            "Combined 1D CNN & RNN Test Loss: 33.88535031847134%\n",
            "Dense Net Conv. Base + Classifier Test Loss: 5.031847133757962%\n",
            "RNN Test Loss: 31.719745222929934%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}