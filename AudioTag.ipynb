{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QColeman97/AudioTagger/blob/master/AudioTag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcgupmmSzS9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e1825365-789a-4835-b8d7-eac77f0f4bdf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns # for data visualization\n",
        "\n",
        "# import IPython\n",
        "# import IPython.display as ipd #To play sound in notebook\n",
        "# import scipy as sci\n",
        "# import wave\n",
        "from pathlib import Path\n",
        "\n",
        "from scipy.fftpack import fft #Fast Fourier Transformation\n",
        "from scipy.io import wavfile\n",
        "\n",
        "import librosa\n",
        "from librosa import display\n",
        "import os\n",
        "import glob\n",
        "\n",
        "input_path = 'drive/My Drive/AudioTaggerData/'\n",
        "\n",
        "\n",
        "# input_path = '../AudioTaggerData/'\n",
        "train_files_path = input_path + 'FSDKaggle2018.audio_train'\n",
        "test_files_path = input_path + 'FSDKaggle2018.audio_test'\n",
        "train_csv_path = (input_path +\n",
        "                  'FSDKaggle2018.meta/train_post_competition.csv')\n",
        "test_csv_path = (input_path +\n",
        "                 'FSDKaggle2018.meta/' +\n",
        "                 'test_post_competition_scoring_clips.csv')\n",
        "\n",
        "#scipy.wavfile.read returns rate of wave, and # of data read\n",
        "filename = '/001ca53d.wav'\n",
        "# sample_rate, samples = wavfile.read(str(audio_train_file) + filename)\n",
        "# print(samples)\n",
        "# print(train.shape)\n",
        "\n",
        "# Data preprocessing part\n",
        "\n",
        "df_train = pd.read_csv(train_csv_path)\n",
        "df_test = pd.read_csv(test_csv_path)\n",
        "\n",
        "unique_labels = df_train.label.unique()\n",
        "num_class = len(unique_labels)\n",
        "label2index = {label: index for index, label in enumerate(unique_labels)}\n",
        "\n",
        "# print('Label to index:')\n",
        "# print(label2index)\n",
        "\n",
        "train_dict = pd.Series.from_csv(train_csv_path, header = 0).to_dict()\n",
        "test_dict = pd.Series.from_csv(test_csv_path, header = 0).to_dict()\n",
        "\n",
        "\n",
        "# print('Train dict at:')\n",
        "# print(test_dict['40f595b9.wav'])\n",
        "# KeyError: '40f595b9.wav'\n",
        "\n",
        "# train_df = pd.read_csv(train_csv_path, header = 0)\n",
        "# train_dict = train_df.transpose().to_dict()\n",
        "# print('train dict:')\n",
        "# print(train_dict)\n",
        "\n",
        "#Indices of manually verified training data\n",
        "# verified_train = np.array(df_train[df_train.manually_verified == 1].index)\n",
        "# print(len(verified_train))\n",
        "# print(len(df_train))\n",
        "\n",
        "#array of labels in number form (0 = hi-hat, 1 = saxophone, etc)\n",
        "label_emb_indices = np.array([label2index[label] for label in df_train.label])\n",
        "\n",
        "label_emb_test_indices = np.array([label2index[label] for label in df_test.label])\n",
        "\n",
        "# print(plain_y_train)\n",
        "\n",
        "# print('Label emb indices:')\n",
        "# print(label_emb_indices)\n",
        "\n",
        "# # Approach X uses longer sound, then it uses suppressed\n",
        "# # confX['sampling_rate'] = 26000\n",
        "# # sampling_rate = 44100 # Original file sr\n",
        "# sampling_rate = 32000\n",
        "# # duration = 4\n",
        "# duration = 5\n",
        "# # confX['hop_length'] = 520  # 20ms\n",
        "# hop_length = 192\n",
        "# fmin = 20\n",
        "# fmax = sampling_rate // 2\n",
        "# # confX['n_mels'] = 48\n",
        "# n_mels = 128\n",
        "# # confX['n_fft'] = confX['n_mels'] * 20\n",
        "# n_fft = 1024\n",
        "# audio_split = 'dont_crop'\n",
        "# samples = sampling_rate * duration\n",
        "# dims = (n_mels, 1 + int(np.floor(samples / hop_length)), 1)\n",
        "\n",
        "\n",
        "def pre_process(pathname):\n",
        "    sampling_rate = 32000\n",
        "    # duration = 4\n",
        "    # duration = 5\n",
        "    # confX['hop_length'] = 520  # 20ms\n",
        "    hop_length = 192\n",
        "    # fmin = 20\n",
        "    # fmax = sampling_rate // 2\n",
        "    fmax = None\n",
        "    # confX['n_mels'] = 48\n",
        "    n_mels = 128 # originally\n",
        "#     n_mels = 32\n",
        "    # confX['n_fft'] = confX['n_mels'] * 20\n",
        "    n_fft = 1024\n",
        "    # audio_split = 'dont_crop'\n",
        "    # samples = sampling_rate * duration\n",
        "    # dims = (n_mels, 1 + int(np.floor(samples / hop_length)), 1)\n",
        "\n",
        "    # TODO: Correct spectrogram creation to minimize bad data\n",
        "\n",
        "    # y, sr = librosa.load(pathname, sr = None)\n",
        "    y, sr = librosa.load(pathname, sr = sampling_rate)\n",
        "    # print('Y len:', len(y))\n",
        "    y, (trim_begin, trim_end) = librosa.effects.trim(y)\n",
        "\n",
        "    # print('Y len:', len(y))\n",
        "#     y = librosa.effects.time_stretch(y, 2)\n",
        "#     print('Y len:', len(y))\n",
        "\n",
        "    # while len(y) > 10000:\n",
        "    #     y = librosa.effects.time_stretch(y, 2)\n",
        "    #     print('Y len:', len(y))\n",
        "    # print()\n",
        "\n",
        "    # Amplitudes of STFT\n",
        "    stft = np.abs(librosa.stft(y, n_fft = n_fft, hop_length = hop_length,\n",
        "                               window = 'hann', center = True,\n",
        "                               pad_mode = 'reflect'))\n",
        "\n",
        "    # print('stft shape:', stft.shape)\n",
        "\n",
        "    freqs = librosa.core.fft_frequencies(sr = sampling_rate, n_fft = n_fft)\n",
        "    stft = librosa.perceptual_weighting(stft*2, freqs, ref = 1.0, amin = 1e-10,\n",
        "                                        top_db = 99.0)\n",
        "\n",
        "    # print('stft shape:', stft.shape)\n",
        "\n",
        "    # Apply mel filterbank\n",
        "    # Power param is set to 2 (power) by default\n",
        "    mel_spect = librosa.feature.melspectrogram(S = stft, sr = sampling_rate,\n",
        "                                               n_mels = n_mels, fmax = fmax)\n",
        "\n",
        "    # print('mel shape:', mel_spect.shape)\n",
        "\n",
        "    log_mel_spect = librosa.core.power_to_db(mel_spect)\n",
        "\n",
        "    # print('log mel shape:', log_mel_spect.shape)\n",
        "\n",
        "    # spectrogram = librosa.feature.melspectrogram(S = stft)\n",
        "    # Keep spectrogram\n",
        "    # return np.asarray(spectrogram)\n",
        "    return np.asarray(log_mel_spect)\n",
        "\n",
        "\n",
        "# pre_process(audio_train_file + filename)\n",
        "def get_data(pathname, training = True):\n",
        "    file_list = glob.glob(os.path.join(pathname, '*.wav'))\n",
        "\n",
        "    if training:\n",
        "        data_f = open('Audio.train', 'w')\n",
        "    else:\n",
        "        data_f = open('Audio.test', 'w')\n",
        "\n",
        "    # print(file_list)\n",
        "#     spectrograms, times = [], []\n",
        "    spectrograms = np.ndarray((9474, 256, 128))\n",
        "\n",
        "    for i, file in enumerate(file_list):\n",
        "        print(\"%04d / %d | %s\" % (i + 1, len(file_list), file))\n",
        "\n",
        "        try:\n",
        "            spectrogram = pre_process(file)\n",
        "        except Exception:\n",
        "            print('Weird, couldnt convert to spectrogram, skipping file')\n",
        "            continue\n",
        "\n",
        "        # times.append(spectrogram.shape[1])\n",
        "\n",
        "        # Originally 2000 (average)\n",
        "#         time_restriction = 64\n",
        "        time_restriction = 256\n",
        "        if time_restriction >= spectrogram.shape[1]:\n",
        "            pad_amount = time_restriction - spectrogram.shape[1]\n",
        "            # Use avg or max time\n",
        "            spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_amount)),\n",
        "                                 'minimum')\n",
        "        else:\n",
        "            spectrogram = spectrogram[:, :time_restriction]\n",
        "\n",
        "        spectrogram = spectrogram.transpose()\n",
        "\n",
        "        # print(\"Spectrogram Shape:\", spectrogram.shape)\n",
        "\n",
        "#         spectrograms.append(spectrogram.astype(np.float32))\n",
        "        for j in range(len(spectrogram)):\n",
        "            for k in range(len(spectrogram[j])):\n",
        "                spectrograms[i][j][k] = spectrogram[j][k].astype(np.float32)\n",
        "                \n",
        "        # data_f.write(np.array2string(spectrogram) + '\\n\\n')\n",
        "#         np.savetxt(data_f, spectrogram)\n",
        "#         data_f.write('\\n')\n",
        "\n",
        "        # if i > 500:\n",
        "        #     break\n",
        "        # if 32 < i < 50:  # 34 is a weird one\n",
        "#         if i % 12 == 0:\n",
        "        \n",
        "#             plt.figure(\"General-Purpose \")\n",
        "#             plt.clf()\n",
        "#             plt.subplots_adjust(right = 0.98, left = 0.1, bottom = 0.1,\n",
        "#                                 top = 0.99)\n",
        "#             plt.imshow(spectrogram, origin = \"lower\",\n",
        "#                        interpolation = \"nearest\", cmap = \"viridis\")\n",
        "#             plt.xlabel(\"%d bins\" % spectrogram.shape[1])\n",
        "#             plt.ylabel(\"%d frames\" % spectrogram.shape[0])\n",
        "#             plt.colorbar()\n",
        "#             plt.show()\n",
        "        \n",
        "            # display.specshow(spectrogram, y_axis = 'log', x_axis = 'time')\n",
        "            #\n",
        "            # plt.title('Mel Spectrogram')\n",
        "            # plt.colorbar(format = '%+2.0f dB')\n",
        "            # plt.tight_layout()\n",
        "            # plt.show()\n",
        "        \n",
        "            # print('Spectrogram:', i)\n",
        "            # print(spectrogram)\n",
        "\n",
        "    # average_time = np.average(times)\n",
        "    # print('Average timesteps:', average_time)\n",
        "    # max_time = np.amax(times)\n",
        "    # print('Max timesteps:', max_time)\n",
        "\n",
        "    return spectrograms\n",
        "#     data_f.close()\n",
        "\n",
        "\n",
        "def get_labels(pathname, training = True):\n",
        "    file_list = glob.glob(os.path.join(pathname, '*.wav'))\n",
        "\n",
        "    if training:\n",
        "        labels_f = open('Labels.train', 'w')\n",
        "    else:\n",
        "        labels_f = open('Labels.test', 'w')\n",
        "\n",
        "#     labels = []\n",
        "    labels = np.ndarray((1570, 41))\n",
        "    for i, file in enumerate(file_list):\n",
        "#         label = np.zeros((41,))\n",
        "        categ = (train_dict[file.split('/')[-1]] if\n",
        "            (training) else test_dict[file.split('/')[-1]])\n",
        "        hot_index = label2index[categ]\n",
        "        labels[i][hot_index] = 1\n",
        "#         labels.append(label)\n",
        "#         labels_f.write(' '.join([str(x) for x in label]) + '\\n')\n",
        "\n",
        "    return np.array(labels)\n",
        "#     labels_f.close()\n",
        "\n",
        "\n",
        "# get_data(train_files_path, training = True)\n",
        "# get_labels(train_files_path, training = True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4141: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
            "  infer_datetime_format=infer_datetime_format)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbbUGp4yGKIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models, layers\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "# import librosa\n",
        "# from librosa import display\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import IPython.display as ipd  # only for IPython notebooks\n",
        "# import pyaudio\n",
        "# import wave\n",
        "# import PreProcess\n",
        "\n",
        "\n",
        "# #1 preprocess method\n",
        "# Normalize input w/ batch normalization (BN)\n",
        "# Input is trnasformed into logmel domain (turned into log-mel spectrograms)\n",
        "# BN again & reshaped (time, frequency, 1) - greyscale image\n",
        "# Then conv stuff\n",
        "\n",
        "# Hyperparams / config to test:\n",
        "# ch 7: (advanced)\n",
        "#   model ensembling (of 2D CNN & Combined 1D CNN & RNN combo, etc??)\n",
        "#   Replace Conv2D w/ SeparableConv2D\n",
        "#   Batch normailization (can replace regularization)\n",
        "#   Inception\n",
        "#   Residual connections\n",
        "#   DenseNet is a good model to follow (top leader) or fine tune from\n",
        "\n",
        "# ch 5: (convnets)\n",
        "#   Tune HP (# neurons, layers, epochs, batch_size)\n",
        "#   dropout, regularization (can be replaced by BN)\n",
        "#   Data augmentation\n",
        "\n",
        "# K-Fold Validation\n",
        "\n",
        "\n",
        "# class LyrParams:\n",
        "#     # Describes, for a layer, # channels/neurons, dropout rate &\n",
        "#     # regularization choice (Lyr = layer)\n",
        "#     def __init__(self, units, dpt = None):\n",
        "#         self.units = units\n",
        "#         self.dpt = dpt\n",
        "\n",
        "# nn.add(layers.Conv2D(prm_lyrs[1].units, (3, 3), activation = 'relu'))\n",
        "# if prm_lyrs[0].dpt:\n",
        "#   nn.add(layers.dpt(prm_lyrs[0].dpt))\n",
        "\n",
        "# Lab 3 is a good one for backup\n",
        "# Model structure after best methods - 6 conv units (conv *2 + maxpool)\n",
        "def make_model(input_shape):\n",
        "    # Current Shape: (500, 128, 1)  # 500 = timesteps, 128 = frequencies\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv2D(64, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu',\n",
        "                                  input_shape = input_shape))\n",
        "    # Shape: (126, 498, 64)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(64, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (124, 496, 64)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (62, 248, 64)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(128, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (60, 246, 128)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(128, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (58, 244, 128)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (29, 122, 128)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "#     # Possibly make this block more like the others\n",
        "#     nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "#                                   activation = 'relu'))\n",
        "#     # Shape: (27, 120, 256)\n",
        "#     nn.add(layers.BatchNormalization())\n",
        "#     nn.add(layers.Dropout(0.3))\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (25, 118, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (23, 116, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (21, 114, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (10, 57, 256)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "#     nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "#                                   activation = 'relu'))\n",
        "#     # Shape: (8, 55, 512)\n",
        "#     nn.add(layers.BatchNormalization())\n",
        "#     nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "#                                   activation = 'relu'))\n",
        "#     # Shape: (6, 53, 512)\n",
        "#     nn.add(layers.BatchNormalization())\n",
        "#     nn.add(layers.MaxPooling2D((2, 2)))\n",
        "#     # Shape: (3, 26, 512)\n",
        "#     nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (1, 24, 512)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "    return nn\n",
        "\n",
        "\n",
        "# Get data\n",
        "# input_path = 'AudioTaggerData/'\n",
        "input_path = 'drive/My Drive/AudioTaggerData/'\n",
        "\n",
        "train_files_path = input_path + 'FSDKaggle2018.audio_train'\n",
        "test_files_path = input_path + 'FSDKaggle2018.audio_test'\n",
        "train_csv_path = (input_path +\n",
        "                  'FSDKaggle2018.meta/train_post_competition.csv')\n",
        "test_csv_path = (input_path +\n",
        "                 'FSDKaggle2018.meta/' +\n",
        "                 'test_post_competition_scoring_clips.csv')\n",
        "\n",
        "\n",
        "def get_train_data():\n",
        "#     data = np.ndarray((9474, 256, 128))\n",
        "    data = np.ndarray((9474, 64, 32))\n",
        "\n",
        "#     data_len, data = 9474, []\n",
        "#     data_len, time_len, freq = 9474, 256, 128\n",
        "    data_len, time_len, freq = 9474, 64, 32\n",
        "\n",
        "\n",
        "#     data_len, data = 3, []\n",
        "    with open(input_path + 'Audio.train', 'r') as data_f:\n",
        "        for i in range(data_len):\n",
        "#             sample = []\n",
        "#             time_step = [float(elem) for elem in\n",
        "#                          data_f.readline().split()]\n",
        "#             print(time_step)\n",
        "            for j in range(time_len):\n",
        "#             while time_step != []:\n",
        "#                 sample.append(np.array(time_step))\n",
        "                \n",
        "    \n",
        "                time_step = [float(elem) for elem in\n",
        "                             data_f.readline().split()]\n",
        "#                 sample.append(time_step)\n",
        "                \n",
        "                if len(time_step) < freq:\n",
        "                    rest = freq - len(time_step)\n",
        "                    time_step += [-100.0 for x in range(rest)]\n",
        "    \n",
        "                for k in range(freq):\n",
        "#                     if (len(time_step) != 32):\n",
        "#                         print('BAD time step len:', i)\n",
        "#                     if (i >= 9474 or j >= 64 or k >= 32):\n",
        "#                         print('OOB error:')\n",
        "#                         print(i,j,k)\n",
        "#                     print(len(time_step))\n",
        "                    data[i][j][k] = time_step[k]\n",
        "            \n",
        "#                 print(time_step)\n",
        "                # print('line')\n",
        "                # print(line)\n",
        "#                 sample.append(np.array(time_step))\n",
        "#             data.append(np.array(sample))\n",
        "#             data.append(sample)\n",
        "\n",
        "            data_f.readline()\n",
        "\n",
        "#             print('DATA:')\n",
        "#             print(data)\n",
        "\n",
        "#     return np.array(data)\n",
        "    return data\n",
        "\n",
        "def get_test_data():\n",
        "#     data = np.ndarray((1570, 256, 128))\n",
        "    data = np.ndarray((1570, 64, 32))\n",
        "    data_len = 1570\n",
        "\n",
        "#     data_len, data = 3, []\n",
        "    with open(input_path + 'Audio.test', 'r') as data_f:\n",
        "        for i in range(data_len):\n",
        "\n",
        "            for j in range(64):\n",
        "    \n",
        "                time_step = [float(elem) for elem in\n",
        "                             data_f.readline().split()]\n",
        "#                 sample.append(time_step)\n",
        "                \n",
        "                if len(time_step) < 32:\n",
        "                    rest = 32 - len(time_step)\n",
        "                    time_step += [-100.0 for x in range(rest)]\n",
        "    \n",
        "                for k in range(32):\n",
        "                    data[i][j][k] = time_step[k]\n",
        "\n",
        "            data_f.readline()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_train_labels():\n",
        "    labels = np.ndarray((9474, 41))\n",
        "#     labels_len, labels = 9474, []\n",
        "    labels_len = 9474\n",
        "\n",
        "#     labels_len, labels = 3, []\n",
        "    with open(input_path + 'Labels.train', 'r') as label_f:\n",
        "        for i in range(labels_len):\n",
        "            label = [float(elem) for elem in label_f.readline().split()]\n",
        "#             labels.append(label)\n",
        "\n",
        "            if len(label) < 41:\n",
        "                    rest = 41 - len(label)\n",
        "                    label += [0.0 for x in range(rest)]\n",
        "            \n",
        "            for j in range(41):\n",
        "                labels[i][j] = label[j]\n",
        "\n",
        "#     return np.array(labels)\n",
        "    return labels\n",
        "\n",
        "def get_test_labels():\n",
        "    labels = np.ndarray((1570, 41))\n",
        "#     labels_len, labels = 9474, []\n",
        "    labels_len = 1570\n",
        "\n",
        "#     labels_len, labels = 3, []\n",
        "    with open(input_path + 'Labels.test', 'r') as label_f:\n",
        "        for i in range(labels_len):\n",
        "            label = [float(elem) for elem in label_f.readline().split()]\n",
        "#             labels.append(label)\n",
        "\n",
        "            if len(label) < 41:\n",
        "                    rest = 41 - len(label)\n",
        "                    label += [0.0 for x in range(rest)]\n",
        "            \n",
        "            for j in range(41):\n",
        "                labels[i][j] = label[j]\n",
        "\n",
        "#     return np.array(labels)\n",
        "    return labels\n",
        "\n",
        "# 1. Create training data (log mel spectrograms)\n",
        "# dummy_samples = 10\n",
        "# Num train samples = 9474\n",
        "# Num test samples = 1570\n",
        "\n",
        "\n",
        "\n",
        "train_data = get_train_data()\n",
        "train_labels = get_train_labels()\n",
        "\n",
        "test_data = get_test_data()\n",
        "test_labels = get_test_labels()\n",
        "# test_data = get_data(test_files_path, training = False)\n",
        "# test_labels = get_labels(test_files_path, training = False)\n",
        "\n",
        "train_samples, test_samples = 9474, 1570\n",
        "max_timesteps = train_data.shape[1]\n",
        "num_freq = train_data.shape[2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ua6ZrXRNkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1805
        },
        "outputId": "842f95a9-b154-40bd-f9c2-c250935a9ba1"
      },
      "source": [
        "mean = train_data.mean(axis = 0)\n",
        "# print('Mean of sample 1:')\n",
        "# print(mean[0])\n",
        "train_data -= mean\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std\n",
        "\n",
        "# ONCE I HAVE TEST DATA\n",
        "test_data -= mean\n",
        "test_data /= std\n",
        "\n",
        "# Use getter methods here\n",
        "# dummy_train_data = np.random.random((dummy_samples, dummy_max_timesteps,\n",
        "#                                      dummy_num_freq, 1))\n",
        "# # Categorical labels\n",
        "# dummy_train_labels = np.zeros((10, 41))\n",
        "# for i, label in enumerate(dummy_train_labels):\n",
        "#     label[i] = 1\n",
        "# print(dummy_train_labels)\n",
        "\n",
        "model = make_model((max_timesteps, num_freq, 1))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    partial_train_data = partial_train_data.reshape(\n",
        "            (partial_train_data.shape[0], \n",
        "             partial_train_data.shape[1], \n",
        "             partial_train_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    val_data = val_data.reshape(\n",
        "            (val_data.shape[0], \n",
        "             val_data.shape[1], \n",
        "             val_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('2DCNN.h5')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 18s 2ms/step - loss: 3.4965 - acc: 0.0909 - val_loss: 3.7601 - val_acc: 0.0769\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 789us/step - loss: 3.1230 - acc: 0.1682 - val_loss: 3.1088 - val_acc: 0.1719\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 793us/step - loss: 2.8794 - acc: 0.2174 - val_loss: 3.3704 - val_acc: 0.1482\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 813us/step - loss: 2.7567 - acc: 0.2522 - val_loss: 3.0092 - val_acc: 0.2171\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 803us/step - loss: 2.6366 - acc: 0.2726 - val_loss: 2.8893 - val_acc: 0.2521\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 2.5481 - acc: 0.2893 - val_loss: 3.3535 - val_acc: 0.2149\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 2.4929 - acc: 0.3021 - val_loss: 2.9302 - val_acc: 0.2576\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 816us/step - loss: 2.4226 - acc: 0.3202 - val_loss: 3.0718 - val_acc: 0.2711\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 827us/step - loss: 2.3572 - acc: 0.3363 - val_loss: 2.8218 - val_acc: 0.2817\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 827us/step - loss: 2.2921 - acc: 0.3483 - val_loss: 2.8442 - val_acc: 0.3036\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 817us/step - loss: 2.3349 - acc: 0.3503 - val_loss: 2.5988 - val_acc: 0.3264\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 2.2723 - acc: 0.3653 - val_loss: 2.5893 - val_acc: 0.3260\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 2.2284 - acc: 0.3676 - val_loss: 2.4128 - val_acc: 0.3437\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 811us/step - loss: 2.2040 - acc: 0.3829 - val_loss: 2.3759 - val_acc: 0.3585\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 2.1678 - acc: 0.3911 - val_loss: 2.6217 - val_acc: 0.3387\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 2.0950 - acc: 0.4094 - val_loss: 2.4490 - val_acc: 0.3442\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 807us/step - loss: 2.1049 - acc: 0.4066 - val_loss: 2.3113 - val_acc: 0.3796\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 805us/step - loss: 2.0436 - acc: 0.4247 - val_loss: 2.5515 - val_acc: 0.3463\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 2.0337 - acc: 0.4226 - val_loss: 2.3549 - val_acc: 0.3628\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 2.0158 - acc: 0.4333 - val_loss: 2.3808 - val_acc: 0.3754\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 2.0421 - acc: 0.4184 - val_loss: 2.0196 - val_acc: 0.4489\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 2.0019 - acc: 0.4333 - val_loss: 1.9465 - val_acc: 0.4540\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 1.9548 - acc: 0.4507 - val_loss: 2.1091 - val_acc: 0.4257\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 813us/step - loss: 1.9598 - acc: 0.4424 - val_loss: 2.1486 - val_acc: 0.4282\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 813us/step - loss: 1.8958 - acc: 0.4610 - val_loss: 2.0729 - val_acc: 0.4493\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 815us/step - loss: 1.8886 - acc: 0.4613 - val_loss: 2.2229 - val_acc: 0.4143\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 1.8654 - acc: 0.4682 - val_loss: 2.1303 - val_acc: 0.4375\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 813us/step - loss: 1.8523 - acc: 0.4724 - val_loss: 2.1957 - val_acc: 0.4329\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 1.8038 - acc: 0.4868 - val_loss: 2.0654 - val_acc: 0.4485\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 1.7808 - acc: 0.4930 - val_loss: 2.2098 - val_acc: 0.4193\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 1.8736 - acc: 0.4662 - val_loss: 1.6877 - val_acc: 0.5207\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 1.8061 - acc: 0.4849 - val_loss: 1.6703 - val_acc: 0.5224\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 1.7860 - acc: 0.4882 - val_loss: 1.8025 - val_acc: 0.4894\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 1.7406 - acc: 0.5042 - val_loss: 1.8538 - val_acc: 0.4878\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 1.7356 - acc: 0.5053 - val_loss: 1.8847 - val_acc: 0.4856\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 1.7123 - acc: 0.5037 - val_loss: 1.7750 - val_acc: 0.5068\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 1.6702 - acc: 0.5186 - val_loss: 1.7677 - val_acc: 0.5051\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 826us/step - loss: 1.6711 - acc: 0.5138 - val_loss: 1.8004 - val_acc: 0.4970\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 811us/step - loss: 1.6347 - acc: 0.5228 - val_loss: 1.8089 - val_acc: 0.4907\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 1.6094 - acc: 0.5363 - val_loss: 1.9210 - val_acc: 0.4713\n",
            "0.34322212888179604 / 2.516559417183335\n",
            "0.3685599660148492 / 2.3287438468353168\n",
            "0.35177364814523104 / 2.423696457534223\n",
            "0.37288851331214645 / 2.346856452323295\n",
            "0.3814400334817332 / 2.367132884425086\n",
            "0.3700380060519721 / 2.450080604166598\n",
            "0.3949535470959302 / 2.284849281246598\n",
            "0.3868243243243243 / 2.4048502010268136\n",
            "0.3959037154107481 / 2.2627678183284967\n",
            "0.3924197635135135 / 2.338964618541099\n",
            "Best val loss: 2.516559417183335 & with acc: 0.34322212888179604 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcabAPVsZpEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "0ca3ada6-dc5c-4130-a128-7d9beb347c9e"
      },
      "source": [
        "\n",
        "\n",
        "def make_combined_CNN_RNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu',\n",
        "                                  input_shape = (None, input_shape[-1])))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling1D(3))\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    # nn.add(layers.MaxPooling1D(3))\n",
        "    # nn.add(layers.Dropout(0.3))\n",
        "    #\n",
        "    # nn.add(layers.SeparableConv1D(512, 5, activation = 'relu'))\n",
        "    # nn.add(layers.BatchNormalization())\n",
        "    # nn.add(layers.SeparableConv1D(512, 5, activation = 'relu'))\n",
        "    # nn.add(layers.BatchNormalization())\n",
        "\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3)))\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "model = make_combined_CNN_RNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('1DCNN_RNN.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-27371e088108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n\u001b[0;32m---> 62\u001b[0;31m                     epochs = 10, validation_data = (val_data, val_labels),)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mhst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UODbmGDa-LH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "3667011a-480f-4afa-ab41-ee24477ad13d"
      },
      "source": [
        "from keras.applications import DenseNet121\n",
        "\n",
        "def make_dense_net_model(conv_base):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(conv_base)\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "    return nn\n",
        "\n",
        "dn_base = DenseNet121(include_top = False,\n",
        "                      input_shape = (max_timesteps, num_freq, 3),\n",
        "                      pooling = 'avg')\n",
        "dn_base.trainable = True\n",
        "# print(dn_base.summary())\n",
        "# Fine-tuning\n",
        "set_trainable = False\n",
        "for layer in dn_base.layers:\n",
        "    if layer.name == 'conv5_block13_0_bn':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "model = make_dense_net_model(dn_base)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    partial_train_data = partial_train_data.reshape(\n",
        "            (partial_train_data.shape[0], \n",
        "             partial_train_data.shape[1], \n",
        "             partial_train_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    partial_train_data = np.repeat(partial_train_data, 3, axis=3)\n",
        "    \n",
        "    val_data = val_data.reshape(\n",
        "            (val_data.shape[0], \n",
        "             val_data.shape[1], \n",
        "             val_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    val_data = np.repeat(val_data, 3, axis=3)\n",
        "#     print('Val Data shape:')\n",
        "#     print(val_data.shape)\n",
        "\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('DenseNetModel.h5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 32s 5ms/step - loss: 3.9103 - acc: 0.1036 - val_loss: 11.3045 - val_acc: 0.0439\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 2s 327us/step - loss: 2.7763 - acc: 0.2734 - val_loss: 10.1244 - val_acc: 0.0536\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 2s 323us/step - loss: 2.2868 - acc: 0.3880 - val_loss: 10.6130 - val_acc: 0.0536\n",
            "Epoch 4/10\n",
            "6144/7106 [========================>.....] - ETA: 0s - loss: 1.8867 - acc: 0.5085"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-659bbcf15fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n\u001b[0;32m---> 64\u001b[0;31m                     epochs = 10, validation_data = (val_data, val_labels),)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mhst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RimUBVXglSku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        },
        "outputId": "8d5a8814-5eb0-4528-d915-59ef0cc0af52"
      },
      "source": [
        "\n",
        "\n",
        "def make_1DCNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu',\n",
        "                                  input_shape = (None, input_shape[-1])))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling1D(5))\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "#     nn.add(layers.SeparableConv1D(128, 7, activation = 'relu'))\n",
        "#     nn.add(layers.BatchNormalization())\n",
        "#     nn.add(layers.SeparableConv1D(128, 7, activation = 'relu'))\n",
        "#     nn.add(layers.BatchNormalization())\n",
        "#     nn.add(layers.MaxPooling1D(5))\n",
        "#     nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.GlobalAveragePooling1D())\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "model = make_1DCNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('1DCNN.h5')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 847us/step - loss: 3.6751 - acc: 0.0712 - val_loss: 3.4487 - val_acc: 0.1174\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 51us/step - loss: 3.3391 - acc: 0.1337 - val_loss: 3.3128 - val_acc: 0.1360\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 3.1926 - acc: 0.1632 - val_loss: 3.2315 - val_acc: 0.1639\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 50us/step - loss: 3.0928 - acc: 0.1765 - val_loss: 3.1615 - val_acc: 0.1723\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 3.0095 - acc: 0.1931 - val_loss: 3.1026 - val_acc: 0.1883\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 2.9357 - acc: 0.2100 - val_loss: 3.0552 - val_acc: 0.1862\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 51us/step - loss: 2.8864 - acc: 0.2204 - val_loss: 3.0285 - val_acc: 0.1938\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 2.8166 - acc: 0.2360 - val_loss: 3.0052 - val_acc: 0.2078\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.7690 - acc: 0.2522 - val_loss: 2.9764 - val_acc: 0.2069\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 2.7181 - acc: 0.2572 - val_loss: 2.9472 - val_acc: 0.2213\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.7774 - acc: 0.2537 - val_loss: 2.6339 - val_acc: 0.2863\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.7169 - acc: 0.2565 - val_loss: 2.6887 - val_acc: 0.2846\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.6685 - acc: 0.2731 - val_loss: 2.6955 - val_acc: 0.2762\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.6291 - acc: 0.2855 - val_loss: 2.6440 - val_acc: 0.2859\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.5922 - acc: 0.2954 - val_loss: 2.6592 - val_acc: 0.2863\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.5608 - acc: 0.2992 - val_loss: 2.6997 - val_acc: 0.2846\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.5265 - acc: 0.3020 - val_loss: 2.6071 - val_acc: 0.2939\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.5026 - acc: 0.3151 - val_loss: 2.5603 - val_acc: 0.3070\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.4644 - acc: 0.3238 - val_loss: 2.5742 - val_acc: 0.3032\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.4466 - acc: 0.3232 - val_loss: 2.6247 - val_acc: 0.3003\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.4840 - acc: 0.3203 - val_loss: 2.3889 - val_acc: 0.3412\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.4531 - acc: 0.3318 - val_loss: 2.2966 - val_acc: 0.3522\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.4297 - acc: 0.3318 - val_loss: 2.3225 - val_acc: 0.3463\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 50us/step - loss: 2.3961 - acc: 0.3432 - val_loss: 2.2865 - val_acc: 0.3547\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.3740 - acc: 0.3472 - val_loss: 2.3046 - val_acc: 0.3539\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.3485 - acc: 0.3546 - val_loss: 2.3217 - val_acc: 0.3387\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.3294 - acc: 0.3562 - val_loss: 2.3385 - val_acc: 0.3488\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.3082 - acc: 0.3614 - val_loss: 2.3527 - val_acc: 0.3429\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 2.2949 - acc: 0.3612 - val_loss: 2.2938 - val_acc: 0.3539\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 50us/step - loss: 2.2786 - acc: 0.3638 - val_loss: 2.2931 - val_acc: 0.3590\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.3146 - acc: 0.3536 - val_loss: 2.1060 - val_acc: 0.4160\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 2.2876 - acc: 0.3638 - val_loss: 2.1075 - val_acc: 0.4084\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 0s 50us/step - loss: 2.2572 - acc: 0.3733 - val_loss: 2.1040 - val_acc: 0.4075\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.2436 - acc: 0.3729 - val_loss: 2.1091 - val_acc: 0.4109\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 2.2196 - acc: 0.3804 - val_loss: 2.1241 - val_acc: 0.4075\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.2052 - acc: 0.3878 - val_loss: 2.1208 - val_acc: 0.4088\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 0s 49us/step - loss: 2.1942 - acc: 0.3863 - val_loss: 2.1637 - val_acc: 0.4020\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.1751 - acc: 0.3854 - val_loss: 2.1145 - val_acc: 0.4088\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 0s 48us/step - loss: 2.1596 - acc: 0.3959 - val_loss: 2.1305 - val_acc: 0.4092\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 0s 47us/step - loss: 2.1343 - acc: 0.4011 - val_loss: 2.1641 - val_acc: 0.3978\n",
            "0.29022381847371925 / 2.6443730995461747\n",
            "0.2952913846317175 / 2.6013829804755546\n",
            "0.2984586137573461 / 2.5884007424921602\n",
            "0.30595439108642375 / 2.5502884710157243\n",
            "0.30901604669319616 / 2.5476357066953503\n",
            "0.30458192507157456 / 2.5493381377812976\n",
            "0.30964949284050913 / 2.534457145510493\n",
            "0.3166173992527498 / 2.5081942000904602\n",
            "0.3183065882405719 / 2.493702083020597\n",
            "0.3195734803338309 / 2.5072969939257645\n",
            "Best val loss: 2.6443730995461747 & with acc: 0.29022381847371925 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCOWN6mzmJBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        },
        "outputId": "e5c7d336-33f7-4e93-bccb-0e43a5d968c7"
      },
      "source": [
        "def make_RNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True,\n",
        "                                            input_shape =\n",
        "                                            (None, input_shape[-1]))))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3)))\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "\n",
        "model = make_RNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('RNN.h5')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 19s 3ms/step - loss: 3.6112 - acc: 0.0671 - val_loss: 3.4713 - val_acc: 0.0887\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 3.4019 - acc: 0.1088 - val_loss: 3.3418 - val_acc: 0.1204\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 3.2729 - acc: 0.1337 - val_loss: 3.2336 - val_acc: 0.1584\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 3.1795 - acc: 0.1621 - val_loss: 3.1622 - val_acc: 0.1736\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 3.1167 - acc: 0.1717 - val_loss: 3.1093 - val_acc: 0.1795\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 3.0486 - acc: 0.1860 - val_loss: 3.0596 - val_acc: 0.1888\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 3.0071 - acc: 0.1939 - val_loss: 3.0253 - val_acc: 0.2052\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.9623 - acc: 0.2008 - val_loss: 2.9881 - val_acc: 0.2145\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.9303 - acc: 0.2121 - val_loss: 2.9648 - val_acc: 0.2099\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.9062 - acc: 0.2145 - val_loss: 2.9374 - val_acc: 0.2196\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.9230 - acc: 0.2094 - val_loss: 2.7425 - val_acc: 0.2538\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.8971 - acc: 0.2140 - val_loss: 2.7372 - val_acc: 0.2593\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.8805 - acc: 0.2163 - val_loss: 2.7336 - val_acc: 0.2568\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.8620 - acc: 0.2222 - val_loss: 2.7108 - val_acc: 0.2601\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.8451 - acc: 0.2190 - val_loss: 2.7012 - val_acc: 0.2597\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.8260 - acc: 0.2274 - val_loss: 2.6867 - val_acc: 0.2648\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.8212 - acc: 0.2276 - val_loss: 2.6903 - val_acc: 0.2656\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.8027 - acc: 0.2325 - val_loss: 2.6569 - val_acc: 0.2800\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.7899 - acc: 0.2377 - val_loss: 2.6503 - val_acc: 0.2791\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.7667 - acc: 0.2395 - val_loss: 2.6357 - val_acc: 0.2787\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.7719 - acc: 0.2435 - val_loss: 2.5976 - val_acc: 0.2821\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.7564 - acc: 0.2411 - val_loss: 2.5824 - val_acc: 0.2876\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.7227 - acc: 0.2515 - val_loss: 2.5894 - val_acc: 0.2867\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.7311 - acc: 0.2473 - val_loss: 2.5916 - val_acc: 0.2766\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.7161 - acc: 0.2536 - val_loss: 2.5855 - val_acc: 0.2766\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.7005 - acc: 0.2557 - val_loss: 2.5672 - val_acc: 0.2880\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.6786 - acc: 0.2647 - val_loss: 2.5555 - val_acc: 0.2990\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.6841 - acc: 0.2595 - val_loss: 2.5518 - val_acc: 0.3045\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.6670 - acc: 0.2651 - val_loss: 2.5515 - val_acc: 0.3028\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.6500 - acc: 0.2677 - val_loss: 2.5379 - val_acc: 0.3015\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.6715 - acc: 0.2627 - val_loss: 2.4769 - val_acc: 0.3100\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 9s 1ms/step - loss: 2.6517 - acc: 0.2701 - val_loss: 2.4839 - val_acc: 0.3074\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.6442 - acc: 0.2670 - val_loss: 2.4619 - val_acc: 0.3180\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.6296 - acc: 0.2768 - val_loss: 2.4783 - val_acc: 0.3095\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.6112 - acc: 0.2802 - val_loss: 2.4745 - val_acc: 0.3108\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.5977 - acc: 0.2762 - val_loss: 2.4732 - val_acc: 0.3125\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.6006 - acc: 0.2779 - val_loss: 2.4696 - val_acc: 0.3146\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.5856 - acc: 0.2861 - val_loss: 2.4693 - val_acc: 0.3201\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.5804 - acc: 0.2858 - val_loss: 2.4538 - val_acc: 0.3159\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 8s 1ms/step - loss: 2.5721 - acc: 0.2868 - val_loss: 2.4509 - val_acc: 0.3239\n",
            "0.23363597998143854 / 2.8220546020043864\n",
            "0.24366554023848996 / 2.7863315359966174\n",
            "0.2549619939480279 / 2.7546317770674422\n",
            "0.25496199324324326 / 2.735717562404839\n",
            "0.25665118243243246 / 2.7176355300722896\n",
            "0.2635135139162476 / 2.696663197633383\n",
            "0.27111486536828244 / 2.685149651926917\n",
            "0.27977195945945943 / 2.6665115002039315\n",
            "0.2769214514945004 / 2.655107361239356\n",
            "0.2809332776311282 / 2.6404546450924227\n",
            "Best val loss: 2.8220546020043864 & with acc: 0.23363597998143854 at epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf8hLixeHaDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reorganize all data as sets of flat vectors of 0-1.0 values\n",
        "# Shape (..., 28, 28, 1)\n",
        "\n",
        "test_data_2D = test_data.reshape(\n",
        "               (test_data.shape[0], \n",
        "                test_data.shape[1], \n",
        "                test_data.shape[2], \n",
        "                1))\n",
        "    \n",
        "test_data_2D_3chan = np.repeat(test_data_2D, 3, axis=3)\n",
        "    \n",
        "\n",
        "cnn2D = models.load_model('2DCNN.h5')\n",
        "rnn = models.load_model('RNN.h5')\n",
        "cnn1D = models.load_model('1DCNN.h5')\n",
        "cnn_rnn = models.load_model('1DCNN_RNN.h5')\n",
        "dense_net = models.load_model('DenseNetModel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6T_IeSRm0ha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "33954387-2d3e-4152-bd25-52c6ce6bbca5"
      },
      "source": [
        "cnn2D_preds = cnn2D.predict(test_data_2D)\n",
        "rnn_preds = rnn.predict(test_data)\n",
        "cnn1D_preds = cnn1D.predict(test_data)\n",
        "cnn_rnn_preds = cnn_rnn.predict(test_data)\n",
        "dense_net_preds = dense_net.predict(test_data_2D_3chan)\n",
        "\n",
        "final_preds = 0.2 * (cnn2D_preds + rnn_preds + cnn1D_preds + \n",
        "                     cnn_rnn_preds + dense_net_preds)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6ef98236e1fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn2D_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrnn_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcnn1D_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn1D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn_rnn_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdense_net_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_2D_3chan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68cI9a2I_sG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3f02e8b4-e0e2-4ad3-a4a1-2ca81d882496"
      },
      "source": [
        "print(final_preds)\n",
        "\n",
        "with open('TestTags.out', 'w') as ttf:\n",
        "    test_file_list = glob.glob(os.path.join(test_files_path, '*.wav'))\n",
        "    for i, filename in enumerate(test_file_list[:1570]):\n",
        "#     for i, res in enumerate(final_preds):\n",
        "        ttf.write(str(i) + ') ' + filename + ' - ' + \n",
        "                  unique_labels[np.argmax(final_preds[i])] + '\\n')\n",
        "#                   unique_labels[final_preds[i].tolist().index(1.)] + '\\n')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.8031243e-02 9.5451373e-04 2.8195424e-02 ... 2.4594639e-03\n",
            "  2.0291019e-02 2.7335644e-02]\n",
            " [1.8031243e-02 9.5451373e-04 2.8195424e-02 ... 2.4594639e-03\n",
            "  2.0291019e-02 2.7335644e-02]\n",
            " [6.2514038e-05 9.0502948e-03 6.6893133e-03 ... 7.8340154e-04\n",
            "  2.1446283e-01 6.3963613e-05]\n",
            " ...\n",
            " [1.2265375e-02 1.5179233e-03 1.5662372e-02 ... 2.8284453e-03\n",
            "  2.4859700e-02 1.9797010e-02]\n",
            " [3.1991460e-04 4.6410818e-02 3.8418453e-02 ... 2.9441938e-03\n",
            "  9.0320796e-02 4.0703779e-04]\n",
            " [9.5287582e-04 3.1554386e-02 2.0984648e-02 ... 2.3450930e-02\n",
            "  1.2461608e-01 4.0440631e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5QoCT-gF8te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "8c696103-355d-4990-ca9a-45151e051b65"
      },
      "source": [
        "cnn2D_loss = cnn2D.evaluate(test_data_2D, test_labels)\n",
        "rnn_loss = rnn.evaluate(test_data, test_labels)\n",
        "cnn1D_loss = cnn1D.evaluate(test_data, test_labels)\n",
        "cnn_rnn_loss = cnn_rnn.evaluate(test_data, test_labels)\n",
        "dense_net_loss = dense_net.evaluate(test_data_2D_3chan, test_labels)\n",
        "\n",
        "print('2DCNN Test Loss:', cnn2D_loss)\n",
        "print('1DCNN Test Loss:', cnn1D_loss)\n",
        "print('Combined 1D CNN & RNN Test Loss:', cnn_rnn_loss)\n",
        "print('Dense Net Conv. Base + Classifier Test Loss:', dense_net_loss)\n",
        "print('RNN Test Loss:', rnn_loss)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4096ff81a24c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn2D_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrnn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcnn1D_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn1D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn_rnn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdense_net_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_2D_3chan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     def predict(self, x,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}