{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioTag.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QColeman97/AudioTagger/blob/master/AudioTag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcgupmmSzS9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "738a7a4d-7141-4cc2-e64e-8003bd9c4ff2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa\n",
        "from librosa import display\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "input_path = 'drive/My Drive/AudioTaggerData/'\n",
        "\n",
        "train_files_path = input_path + 'FSDKaggle2018.audio_train'\n",
        "test_files_path = input_path + 'FSDKaggle2018.audio_test'\n",
        "train_csv_path = (input_path +\n",
        "                  'FSDKaggle2018.meta/train_post_competition.csv')\n",
        "test_csv_path = (input_path +\n",
        "                 'FSDKaggle2018.meta/' +\n",
        "                 'test_post_competition_scoring_clips.csv')\n",
        "\n",
        "# Data preprocessing part\n",
        "\n",
        "df_train = pd.read_csv(train_csv_path)\n",
        "df_test = pd.read_csv(test_csv_path)\n",
        "\n",
        "unique_labels = df_train.label.unique()\n",
        "num_class = len(unique_labels)\n",
        "\n",
        "label2index = {label: index for index, label in enumerate(unique_labels)}\n",
        "\n",
        "train_dict = pd.Series.from_csv(train_csv_path, header = 0).to_dict()\n",
        "test_dict = pd.Series.from_csv(test_csv_path, header = 0).to_dict()\n",
        "\n",
        "#array of labels in number form (0 = hi-hat, 1 = saxophone, etc)\n",
        "label_emb_indices = np.array([label2index[label] for label in df_train.label])\n",
        "\n",
        "label_emb_test_indices = np.array([label2index[label] for label in df_test.label])\n",
        "\n",
        "\n",
        "def pre_process(pathname):\n",
        "    sampling_rate = 32000\n",
        "    hop_length = 192\n",
        "    fmax = None\n",
        "    n_mels = 128\n",
        "    n_fft = 1024\n",
        "\n",
        "    y, sr = librosa.load(pathname, sr = sampling_rate)\n",
        "    # Trim quiet noise away\n",
        "    y, (trim_begin, trim_end) = librosa.effects.trim(y)\n",
        "\n",
        "    # Amplitudes of STFT\n",
        "    stft = np.abs(librosa.stft(y, n_fft = n_fft, hop_length = hop_length,\n",
        "                               window = 'hann', center = True,\n",
        "                               pad_mode = 'reflect'))\n",
        "\n",
        "    freqs = librosa.core.fft_frequencies(sr = sampling_rate, n_fft = n_fft)\n",
        "    stft = librosa.perceptual_weighting(stft*2, freqs, ref = 1.0, amin = 1e-10,\n",
        "                                        top_db = 99.0)\n",
        "\n",
        "    # Apply mel filterbank\n",
        "    # Power param is set to 2 (power) by default\n",
        "    mel_spect = librosa.feature.melspectrogram(S = stft, sr = sampling_rate,\n",
        "                                               n_mels = n_mels, fmax = fmax)\n",
        "\n",
        "    log_mel_spect = librosa.core.power_to_db(mel_spect)\n",
        "\n",
        "    return np.asarray(log_mel_spect)\n",
        "\n",
        "\n",
        "# Get data as a numpy array\n",
        "def get_data(pathname, training = True):\n",
        "    file_list = glob.glob(os.path.join(pathname, '*.wav'))\n",
        "\n",
        "    if training:\n",
        "        data_f = open('Audio.train', 'w')\n",
        "    else:\n",
        "        data_f = open('Audio.test', 'w')\n",
        "\n",
        "    spectrograms = np.ndarray((9474, 256, 128))\n",
        "\n",
        "    for i, file in enumerate(file_list):\n",
        "        print(\"%04d / %d | %s\" % (i + 1, len(file_list), file))\n",
        "\n",
        "        spectrogram = pre_process(file)\n",
        "\n",
        "        time_restriction = 256\n",
        "        if time_restriction >= spectrogram.shape[1]:\n",
        "            pad_amount = time_restriction - spectrogram.shape[1]\n",
        "            spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_amount)),\n",
        "                                 'minimum')\n",
        "        else:\n",
        "            spectrogram = spectrogram[:, :time_restriction]\n",
        "\n",
        "        spectrogram = spectrogram.transpose()\n",
        "\n",
        "        for j in range(len(spectrogram)):\n",
        "            for k in range(len(spectrogram[j])):\n",
        "                spectrograms[i][j][k] = spectrogram[j][k].astype(np.float32)\n",
        "\n",
        "\n",
        "        # Plot every 12th spectrogram\n",
        "        if i % 12 == 0:\n",
        "        \n",
        "            plt.figure(\"General-Purpose \")\n",
        "            plt.clf()\n",
        "            plt.subplots_adjust(right = 0.98, left = 0.1, bottom = 0.1,\n",
        "                                top = 0.99)\n",
        "            plt.imshow(spectrogram, origin = \"lower\",\n",
        "                       interpolation = \"nearest\", cmap = \"viridis\")\n",
        "            plt.xlabel(\"%d bins\" % spectrogram.shape[1])\n",
        "            plt.ylabel(\"%d frames\" % spectrogram.shape[0])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "        \n",
        "            display.specshow(spectrogram, y_axis = 'log', x_axis = 'time')\n",
        "            \n",
        "            plt.title('Mel Spectrogram')\n",
        "            plt.colorbar(format = '%+2.0f dB')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        \n",
        "            print('Spectrogram:', i)\n",
        "            print(spectrogram)\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "\n",
        "# Get labels as a numpy array\n",
        "def get_labels(pathname, training = True):\n",
        "    file_list = glob.glob(os.path.join(pathname, '*.wav'))\n",
        "\n",
        "    if training:\n",
        "        labels_f = open('Labels.train', 'w')\n",
        "    else:\n",
        "        labels_f = open('Labels.test', 'w')\n",
        "\n",
        "    labels = np.ndarray((1570, 41))\n",
        "    for i, file in enumerate(file_list):\n",
        "        categ = (train_dict[file.split('/')[-1]] if\n",
        "            (training) else test_dict[file.split('/')[-1]])\n",
        "        hot_index = label2index[categ]\n",
        "        labels[i][hot_index] = 1\n",
        "\n",
        "    return np.array(labels)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4141: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
            "  infer_datetime_format=infer_datetime_format)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbbUGp4yGKIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24df8a31-2c19-4860-a2cf-92931bd8a2e5"
      },
      "source": [
        "from keras import models, layers\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Model structure after best methods ~ 6 conv units (conv *2 + maxpool)\n",
        "def make_model(input_shape):\n",
        "    # Example Shape: (500, 128, 1)  # 500 = timesteps, 128 = frequencies\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv2D(64, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu',\n",
        "                                  input_shape = input_shape))\n",
        "    # Shape: (126, 498, 64)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(64, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (124, 496, 64)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (62, 248, 64)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(128, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (60, 246, 128)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(128, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (58, 244, 128)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (29, 122, 128)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (25, 118, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (23, 116, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "    nn.add(layers.SeparableConv2D(256, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (21, 114, 256)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling2D((2, 2)))\n",
        "    # Shape: (10, 57, 256)\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    # Shape: (1, 24, 512)\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv2D(512, (3, 3), padding = 'same',\n",
        "                                  activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "    return nn\n",
        "\n",
        "\n",
        "# Get data\n",
        "input_path = 'drive/My Drive/AudioTaggerData/'\n",
        "\n",
        "train_files_path = input_path + 'FSDKaggle2018.audio_train'\n",
        "test_files_path = input_path + 'FSDKaggle2018.audio_test'\n",
        "train_csv_path = (input_path +\n",
        "                  'FSDKaggle2018.meta/train_post_competition.csv')\n",
        "test_csv_path = (input_path +\n",
        "                 'FSDKaggle2018.meta/' +\n",
        "                 'test_post_competition_scoring_clips.csv')\n",
        "\n",
        "\n",
        "def get_train_data():\n",
        "#   Possible future better shape (9474, 256, 128))\n",
        "    data = np.ndarray((9474, 64, 32))\n",
        "\n",
        "    data_len, time_len, freq = 9474, 64, 32\n",
        "\n",
        "\n",
        "    with open(input_path + 'Audio.train', 'r') as data_f:\n",
        "        for i in range(data_len):\n",
        "\n",
        "            for j in range(time_len):\n",
        "    \n",
        "                time_step = [float(elem) for elem in\n",
        "                             data_f.readline().split()]\n",
        "                \n",
        "                if len(time_step) < freq:\n",
        "                    rest = freq - len(time_step)\n",
        "                    time_step += [-100.0 for x in range(rest)]\n",
        "    \n",
        "                for k in range(freq):\n",
        "                    data[i][j][k] = time_step[k]\n",
        "\n",
        "            data_f.readline()\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_test_data():\n",
        "#   Possible future better shape (9474, 256, 128))\n",
        "    data = np.ndarray((1570, 64, 32))\n",
        "    data_len = 1570\n",
        "\n",
        "    with open(input_path + 'Audio.test', 'r') as data_f:\n",
        "        for i in range(data_len):\n",
        "\n",
        "            for j in range(64):\n",
        "    \n",
        "                time_step = [float(elem) for elem in\n",
        "                             data_f.readline().split()]\n",
        "                \n",
        "                if len(time_step) < 32:\n",
        "                    rest = 32 - len(time_step)\n",
        "                    time_step += [-100.0 for x in range(rest)]\n",
        "    \n",
        "                for k in range(32):\n",
        "                    data[i][j][k] = time_step[k]\n",
        "\n",
        "            data_f.readline()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_train_labels():\n",
        "    labels = np.ndarray((9474, 41))\n",
        "    labels_len = 9474\n",
        "\n",
        "    with open(input_path + 'Labels.train', 'r') as label_f:\n",
        "        for i in range(labels_len):\n",
        "            label = [float(elem) for elem in label_f.readline().split()]\n",
        "\n",
        "            if len(label) < 41:\n",
        "                    rest = 41 - len(label)\n",
        "                    label += [0.0 for x in range(rest)]\n",
        "            \n",
        "            for j in range(41):\n",
        "                labels[i][j] = label[j]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def get_test_labels():\n",
        "    labels = np.ndarray((1570, 41))\n",
        "    labels_len = 1570\n",
        "\n",
        "    with open(input_path + 'Labels.test', 'r') as label_f:\n",
        "        for i in range(labels_len):\n",
        "            label = [float(elem) for elem in label_f.readline().split()]\n",
        "\n",
        "            if len(label) < 41:\n",
        "                    rest = 41 - len(label)\n",
        "                    label += [0.0 for x in range(rest)]\n",
        "            \n",
        "            for j in range(41):\n",
        "                labels[i][j] = label[j]\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "train_data = get_train_data()\n",
        "train_labels = get_train_labels()\n",
        "\n",
        "test_data = get_test_data()\n",
        "test_labels = get_test_labels()\n",
        "\n",
        "\n",
        "train_samples, test_samples = 9474, 1570\n",
        "max_timesteps = train_data.shape[1]\n",
        "num_freq = train_data.shape[2]\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ua6ZrXRNkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1482
        },
        "outputId": "8699277a-f5e1-41f9-badb-09cd9231b11d"
      },
      "source": [
        "mean = train_data.mean(axis = 0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std\n",
        "\n",
        "\n",
        "model = make_model((max_timesteps, num_freq, 1))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    partial_train_data = partial_train_data.reshape(\n",
        "            (partial_train_data.shape[0], \n",
        "             partial_train_data.shape[1], \n",
        "             partial_train_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    val_data = val_data.reshape(\n",
        "            (val_data.shape[0], \n",
        "             val_data.shape[1], \n",
        "             val_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('2DCNN.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 19s 3ms/step - loss: 3.4418 - acc: 0.0925 - val_loss: 3.6925 - val_acc: 0.0971\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 835us/step - loss: 3.0871 - acc: 0.1665 - val_loss: 3.4096 - val_acc: 0.1351\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 850us/step - loss: 2.8939 - acc: 0.2167 - val_loss: 3.1613 - val_acc: 0.1921\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 826us/step - loss: 2.7653 - acc: 0.2409 - val_loss: 2.9722 - val_acc: 0.2230\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 822us/step - loss: 2.7221 - acc: 0.2520 - val_loss: 2.8619 - val_acc: 0.2593\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 2.5743 - acc: 0.2815 - val_loss: 2.7974 - val_acc: 0.2652\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 813us/step - loss: 2.5095 - acc: 0.3057 - val_loss: 2.8739 - val_acc: 0.2538\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 2.4422 - acc: 0.3216 - val_loss: 2.7296 - val_acc: 0.2791\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 805us/step - loss: 2.3822 - acc: 0.3359 - val_loss: 2.8066 - val_acc: 0.2762\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 806us/step - loss: 2.3662 - acc: 0.3386 - val_loss: 2.7383 - val_acc: 0.3163\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 807us/step - loss: 2.3691 - acc: 0.3448 - val_loss: 2.4637 - val_acc: 0.3307\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 2.2946 - acc: 0.3525 - val_loss: 2.7179 - val_acc: 0.3125\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 2.2773 - acc: 0.3635 - val_loss: 2.6318 - val_acc: 0.3159\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 2.2268 - acc: 0.3743 - val_loss: 2.4775 - val_acc: 0.3552\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 2.1730 - acc: 0.3881 - val_loss: 2.4082 - val_acc: 0.3649\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 814us/step - loss: 2.1584 - acc: 0.3895 - val_loss: 2.4735 - val_acc: 0.3573\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 815us/step - loss: 2.1373 - acc: 0.3930 - val_loss: 2.4170 - val_acc: 0.3628\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 2.1057 - acc: 0.4102 - val_loss: 2.7106 - val_acc: 0.3404\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 816us/step - loss: 2.0725 - acc: 0.4092 - val_loss: 2.4614 - val_acc: 0.3606\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 811us/step - loss: 2.0560 - acc: 0.4191 - val_loss: 2.4387 - val_acc: 0.3644\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 811us/step - loss: 2.0790 - acc: 0.4080 - val_loss: 2.2380 - val_acc: 0.4008\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 2.0355 - acc: 0.4251 - val_loss: 2.0488 - val_acc: 0.4185\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 2.0026 - acc: 0.4274 - val_loss: 2.2522 - val_acc: 0.3995\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 1.9772 - acc: 0.4444 - val_loss: 2.3010 - val_acc: 0.3919\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 1.9662 - acc: 0.4434 - val_loss: 2.0954 - val_acc: 0.4227\n",
            "Epoch 6/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 1.9144 - acc: 0.4548 - val_loss: 2.1194 - val_acc: 0.4177\n",
            "Epoch 7/10\n",
            "7106/7106 [==============================] - 6s 811us/step - loss: 1.8902 - acc: 0.4610 - val_loss: 2.3812 - val_acc: 0.3826\n",
            "Epoch 8/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 1.8888 - acc: 0.4596 - val_loss: 2.1498 - val_acc: 0.4075\n",
            "Epoch 9/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 1.8624 - acc: 0.4634 - val_loss: 2.2504 - val_acc: 0.4160\n",
            "Epoch 10/10\n",
            "7106/7106 [==============================] - 6s 811us/step - loss: 1.8157 - acc: 0.4834 - val_loss: 2.1565 - val_acc: 0.4126\n",
            "Train on 7106 samples, validate on 2368 samples\n",
            "Epoch 1/10\n",
            "7106/7106 [==============================] - 6s 808us/step - loss: 1.9031 - acc: 0.4541 - val_loss: 1.9524 - val_acc: 0.4535\n",
            "Epoch 2/10\n",
            "7106/7106 [==============================] - 6s 811us/step - loss: 1.8678 - acc: 0.4669 - val_loss: 1.8935 - val_acc: 0.4802\n",
            "Epoch 3/10\n",
            "7106/7106 [==============================] - 6s 809us/step - loss: 1.8045 - acc: 0.4835 - val_loss: 1.8719 - val_acc: 0.4785\n",
            "Epoch 4/10\n",
            "7106/7106 [==============================] - 6s 812us/step - loss: 1.8025 - acc: 0.4790 - val_loss: 1.8768 - val_acc: 0.4662\n",
            "Epoch 5/10\n",
            "7106/7106 [==============================] - 6s 810us/step - loss: 1.7525 - acc: 0.4965 - val_loss: 1.8835 - val_acc: 0.4759\n",
            "Epoch 6/10\n",
            "2048/7106 [=======>......................] - ETA: 3s - loss: 1.6789 - acc: 0.5122"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcabAPVsZpEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def make_combined_CNN_RNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu',\n",
        "                                  input_shape = (None, input_shape[-1])))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling1D(3))\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(128, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3)))\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "model = make_combined_CNN_RNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('1DCNN_RNN.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UODbmGDa-LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import DenseNet121\n",
        "\n",
        "def make_dense_net_model(conv_base):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(conv_base)\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "    return nn\n",
        "\n",
        "dn_base = DenseNet121(include_top = False,\n",
        "                      input_shape = (max_timesteps, num_freq, 3),\n",
        "                      pooling = 'avg')\n",
        "dn_base.trainable = True\n",
        "# print(dn_base.summary())\n",
        "# Fine-tuning\n",
        "set_trainable = False\n",
        "for layer in dn_base.layers:\n",
        "    if layer.name == 'conv5_block13_0_bn':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "model = make_dense_net_model(dn_base)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    partial_train_data = partial_train_data.reshape(\n",
        "            (partial_train_data.shape[0], \n",
        "             partial_train_data.shape[1], \n",
        "             partial_train_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    partial_train_data = np.repeat(partial_train_data, 3, axis=3)\n",
        "    \n",
        "    val_data = val_data.reshape(\n",
        "            (val_data.shape[0], \n",
        "             val_data.shape[1], \n",
        "             val_data.shape[2], \n",
        "             1))\n",
        "    \n",
        "    val_data = np.repeat(val_data, 3, axis=3)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('DenseNetModel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RimUBVXglSku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def make_1DCNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu',\n",
        "                                  input_shape = (None, input_shape[-1])))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(64, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.MaxPooling1D(5))\n",
        "    nn.add(layers.Dropout(0.3))\n",
        "\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.SeparableConv1D(128, 5, activation = 'relu'))\n",
        "    nn.add(layers.BatchNormalization())\n",
        "    nn.add(layers.GlobalAveragePooling1D())\n",
        "\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "model = make_1DCNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('1DCNN.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCOWN6mzmJBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_RNN_model(input_shape):\n",
        "    nn = models.Sequential()\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True,\n",
        "                                            input_shape =\n",
        "                                            (None, input_shape[-1]))))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3,\n",
        "                                            return_sequences = True)))\n",
        "    nn.add(layers.Bidirectional(layers.LSTM(64, dropout = 0.3,\n",
        "                                            recurrent_dropout = 0.3)))\n",
        "    nn.add(layers.Dense(41, activation = 'softmax'))\n",
        "\n",
        "    return nn\n",
        "\n",
        "\n",
        "model = make_RNN_model((max_timesteps, num_freq))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "k = 4\n",
        "num_val = len(train_data) // k\n",
        "num_train = len(train_labels) - num_val\n",
        "all_val_acc_histories, all_val_loss_histories = [], []\n",
        "for x in range(k):\n",
        "    val_data = train_data[x * num_val: (x + 1) * num_val]\n",
        "    val_labels = train_labels[x * num_val: (x + 1) * num_val]\n",
        "\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[: x * num_val], train_data[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    partial_train_labels = np.concatenate(\n",
        "        [train_labels[: x * num_val],\n",
        "         train_labels[(x + 1) * num_val:]],\n",
        "        axis = 0)\n",
        "    \n",
        "    hst = model.fit(partial_train_data, partial_train_labels, batch_size = 1024,\n",
        "                    epochs = 10, validation_data = (val_data, val_labels),)\n",
        "\n",
        "    hst = hst.history\n",
        "    all_val_loss_histories.append(hst['val_loss'])\n",
        "    all_val_acc_histories.append(hst['val_acc'])\n",
        "\n",
        "avg_val_loss_hst = np.mean(all_val_loss_histories, axis = 0)\n",
        "avg_val_acc_hst = np.mean(all_val_acc_histories, axis = 0)\n",
        "\n",
        "best_loss, best_acc, prev_acc, best_epoch = None, None, None, 0\n",
        "\n",
        "acc_increased = True\n",
        "for i in range(10):\n",
        "    print(avg_val_acc_hst[i], '/', avg_val_loss_hst[i])\n",
        "\n",
        "    if prev_acc is not None and avg_val_acc_hst[i] < prev_acc:\n",
        "        acc_increased = False\n",
        "    prev_acc = avg_val_loss_hst[i]\n",
        "\n",
        "    if (best_acc is None or avg_val_acc_hst[i] > best_acc and\n",
        "            acc_increased):\n",
        "        best_acc = avg_val_acc_hst[i]\n",
        "        best_loss = avg_val_loss_hst[i]\n",
        "        best_epoch = i + 1\n",
        "\n",
        "print('Best val loss:', best_loss, '& with acc:', best_acc, 'at epoch:',\n",
        "      str(best_epoch))\n",
        "\n",
        "model.save('RNN.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf8hLixeHaDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_2D = test_data.reshape(\n",
        "               (test_data.shape[0], \n",
        "                test_data.shape[1], \n",
        "                test_data.shape[2], \n",
        "                1))\n",
        "    \n",
        "test_data_2D_3chan = np.repeat(test_data_2D, 3, axis=3)\n",
        "    \n",
        "\n",
        "cnn2D = models.load_model('2DCNN.h5')\n",
        "rnn = models.load_model('RNN.h5')\n",
        "cnn1D = models.load_model('1DCNN.h5')\n",
        "cnn_rnn = models.load_model('1DCNN_RNN.h5')\n",
        "dense_net = models.load_model('DenseNetModel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6T_IeSRm0ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn2D_preds = cnn2D.predict(test_data_2D)\n",
        "rnn_preds = rnn.predict(test_data)\n",
        "cnn1D_preds = cnn1D.predict(test_data)\n",
        "cnn_rnn_preds = cnn_rnn.predict(test_data)\n",
        "dense_net_preds = dense_net.predict(test_data_2D_3chan)\n",
        "\n",
        "final_preds = 0.2 * (cnn2D_preds + rnn_preds + cnn1D_preds + \n",
        "                     cnn_rnn_preds + dense_net_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68cI9a2I_sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(final_preds)\n",
        "\n",
        "with open('TestTags.out', 'w') as ttf:\n",
        "    test_file_list = glob.glob(os.path.join(test_files_path, '*.wav'))\n",
        "    for i, filename in enumerate(test_file_list[:1570]):\n",
        "        ttf.write(str(i) + ') ' + filename.split('/')[-1] + ' ' + \n",
        "                  unique_labels[np.argmax(final_preds[i])] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5QoCT-gF8te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn2D_loss, cnn2D_acc = cnn2D.evaluate(test_data_2D, test_labels)\n",
        "rnn_loss, rnn_acc = rnn.evaluate(test_data, test_labels)\n",
        "cnn1D_loss, cnn1D_acc = cnn1D.evaluate(test_data, test_labels)\n",
        "cnn_rnn_loss, cnn_rnn_acc = cnn_rnn.evaluate(test_data, test_labels)\n",
        "dense_net_loss, dense_net_acc = dense_net.evaluate(test_data_2D_3chan, test_labels)\n",
        "\n",
        "print('2DCNN Test Loss:', str(cnn2D_acc*100) + '%')\n",
        "print('1DCNN Test Loss:', str(cnn1D_acc*100) + '%')\n",
        "print('Combined 1D CNN & RNN Test Loss:', str(cnn_rnn_acc*100) + '%')\n",
        "print('Dense Net Conv. Base + Classifier Test Loss:', str(dense_net_acc*100) + '%')\n",
        "print('RNN Test Loss:', str(rnn_acc*100) + '%')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}